{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "微调一个 Marian 翻译模型进行汉英翻译\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备数据\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建数据集\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'english': 'Slowly and not without struggle, America began to listen.',\n",
       " 'chinese': '美国缓慢地开始倾听，但并非没有艰难曲折。'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, random_split\n",
    "import json\n",
    "\n",
    "max_dataset_size = 22000\n",
    "train_set_size = 20000\n",
    "valid_set_size = 2000\n",
    "\n",
    "\n",
    "class TRANS(Dataset):\n",
    "    def __init__(self, data_file):\n",
    "        self.data = self.load_data(data_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "    def load_data(self, data_file):\n",
    "        data = {}\n",
    "        with open(data_file, \"rt\") as f:\n",
    "            for idx, line in enumerate(f):\n",
    "                if idx == max_dataset_size:\n",
    "                    break\n",
    "                sample = json.loads(line.strip())\n",
    "                data[idx] = sample\n",
    "        return data\n",
    "\n",
    "\n",
    "dataset = TRANS(\"translation2019zh/translation2019zh_train.json\")\n",
    "train_dataset, valid_dataset = random_split(dataset, [train_set_size, valid_set_size])\n",
    "test_dataset = TRANS(\"translation2019zh/translation2019zh_valid.json\")\n",
    "\n",
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理\n",
    "\n",
    "将每一个 batch 中的数据处理为该模型可接受的格式：一个包含 'attention_mask'、'input_ids'、'labels' 和 'decoder_input_ids' 键的字典。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "默认情况下分词器会采用源语言的设定来编码文本，要编码目标语言则需要通过上下文管理器 as_target_tokenizer()：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于翻译任务，标签序列就是目标语言的 token ID 序列。与序列标注任务类似，在模型预测出的标签序列与答案标签序列之间计算损失来调整模型参数，因此同样需要将填充的 pad 字符设置为 -100，以便在使用交叉熵计算序列损失时将它们忽略：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解码器通常需要两种类型的输入：\n",
    "\n",
    "Encoder 的输出：这些是编码器处理输入数据后的一系列隐藏状态，它们为解码器提供了有关输入序列的上下文信息。\n",
    "\n",
    "decoder input IDs 是标签序列的移位，在序列的开始位置增加了一个特殊的“序列起始符”。\n",
    "在训练过程中，模型会基于 decoder input IDs 和 attention mask 来确保在预测某个 token 时不会使用到该 token 及其之后的 token 的信息。考虑到不同模型的移位操作可能存在差异，我们通过模型自带的 `prepare_decoder_input_ids_from_labels` 函数来完成。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages/transformers/models/marian/tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'decoder_input_ids', 'labels'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3892: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-zh-en\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "def collote_fn(batch_samples):\n",
    "    batch_inputs, batch_targets = [], []\n",
    "    for sample in batch_samples:\n",
    "        batch_inputs.append(sample[\"chinese\"])\n",
    "        batch_targets.append(sample[\"english\"])\n",
    "    batch_data = tokenizer(\n",
    "        batch_inputs,\n",
    "        padding=True,\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            batch_targets,\n",
    "            padding=True,\n",
    "            max_length=max_target_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )[\"input_ids\"]\n",
    "        batch_data[\"decoder_input_ids\"] = model.prepare_decoder_input_ids_from_labels(\n",
    "            labels\n",
    "        )\n",
    "        end_token_index = torch.where(labels == tokenizer.eos_token_id)[1]\n",
    "        for idx, end_idx in enumerate(end_token_index):\n",
    "            labels[idx][end_idx + 1 :] = -100\n",
    "        batch_data[\"labels\"] = labels\n",
    "    return batch_data\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=32, shuffle=True, collate_fn=collote_fn\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset, batch_size=32, shuffle=False, collate_fn=collote_fn\n",
    ")\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "print(batch.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化模型参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(data_loader, model, optimizer, lr_scheduler, epoch, total_loss):\n",
    "    model.train()\n",
    "    for batch, batch_data in enumerate(data_loader, start=1):\n",
    "        batch_data = batch_data.to(device)\n",
    "        outputs = model(**batch_data)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证/测试循环负责评估模型的性能。使用 BLEU 用于度量两个词语序列之间的一致性。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu.metrics import BLEU\n",
    "import numpy as np\n",
    "import tqdm.auto as tqdm\n",
    "\n",
    "bleu = BLEU()\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model):\n",
    "    preds, labels = [], []\n",
    "\n",
    "    model.eval()\n",
    "    for batch_data in tqdm(dataloader):\n",
    "        batch_data = batch_data.to(device)\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = (\n",
    "                model.generate(\n",
    "                    batch_data[\"input_ids\"],\n",
    "                    attention_mask=batch_data[\"attention_mask\"],\n",
    "                    max_length=max_target_length,\n",
    "                )\n",
    "                .cpu()\n",
    "                .numpy()\n",
    "            )\n",
    "\n",
    "        decoded_preds = tokenizer.batch_decode(\n",
    "            generated_tokens, skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        label_tokens = batch_data[\"labels\"].cpu().numpy()\n",
    "        label_tokens = np.where(\n",
    "            label_tokens != -100, label_tokens, tokenizer.pad_token_id\n",
    "        )  # 将标签序列中的 -100 替换为 pad token ID 以便于分词器解码\n",
    "        decoded_labels = tokenizer.batch_decode(label_tokens, skip_special_tokens=True)\n",
    "\n",
    "        preds += [pred.strip() for pred in decoded_preds]\n",
    "        labels += [[label.strip()] for label in decoded_labels]\n",
    "    bleu_score = bleu.corpus_score(preds, labels).score\n",
    "    print(f\"BLEU: {bleu_score:>0.2f}\\n\")\n",
    "    return bleu_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在开始训练之前，先评估一下没有微调的模型在测试集上的性能。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = TRANS(\"translation2019zh/translation2019zh_valid.json\")\n",
    "test_dataloader = DataLoader(\n",
    "    test_data, batch_size=32, shuffle=False, collate_fn=collote_fn\n",
    ")\n",
    "batch = next(iter(test_dataloader))\n",
    "print(batch.keys())\n",
    "print(\"batch shape:\", {k: v.shape for k, v in batch.items()})\n",
    "print(batch)\n",
    "\n",
    "test_loop(test_dataloader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 保存模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "learning_rate = 2e-5\n",
    "epoch_num = 3\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=epoch_num * len(train_dataloader),\n",
    ")\n",
    "total_loss = 0.0\n",
    "best_bleu = 0.0\n",
    "\n",
    "for t in range(epoch_num):\n",
    "    total_loss = train_loop(\n",
    "        train_dataloader, model, optimizer, lr_scheduler, t + 1, total_loss\n",
    "    )\n",
    "    valid_bleu = test_loop(valid_dataloader, model)\n",
    "    if valid_bleu > best_bleu:\n",
    "        best_bleu = valid_bleu\n",
    "        torch.save(\n",
    "            model.state_dict(),\n",
    "            f\"epoch_{t+1}_valid_bleu_{valid_bleu:0.2f}_model_weights.bin\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 测试模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = TRANS(\"translation2019zh/translation2019zh_valid.json\")\n",
    "test_dataloader = DataLoader(\n",
    "    test_data, batch_size=32, shuffle=False, collate_fn=collote_fn\n",
    ")\n",
    "\n",
    "import json\n",
    "\n",
    "model.load_state_dict(torch.load(\"epoch_1_valid_bleu_53.38_model_weights.bin\"))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(\"evaluating on test set...\")\n",
    "    sources, preds, labels = [], [], []\n",
    "    for batch_data in tqdm(test_dataloader):\n",
    "        batch_data = batch_data.to(device)\n",
    "        generated_tokens = (\n",
    "            model.generate(\n",
    "                batch_data[\"input_ids\"],\n",
    "                attention_mask=batch_data[\"attention_mask\"],\n",
    "                max_length=max_target_length,\n",
    "            )\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "        )\n",
    "        label_tokens = batch_data[\"labels\"].cpu().numpy()\n",
    "\n",
    "        decoded_sources = tokenizer.batch_decode(\n",
    "            batch_data[\"input_ids\"].cpu().numpy(),\n",
    "            skip_special_tokens=True,\n",
    "            use_source_tokenizer=True,\n",
    "        )\n",
    "        decoded_preds = tokenizer.batch_decode(\n",
    "            generated_tokens, skip_special_tokens=True\n",
    "        )\n",
    "        label_tokens = np.where(\n",
    "            label_tokens != -100, label_tokens, tokenizer.pad_token_id\n",
    "        )\n",
    "        decoded_labels = tokenizer.batch_decode(label_tokens, skip_special_tokens=True)\n",
    "\n",
    "        sources += [source.strip() for source in decoded_sources]\n",
    "        preds += [pred.strip() for pred in decoded_preds]\n",
    "        labels += [[label.strip()] for label in decoded_labels]\n",
    "    bleu_score = bleu.corpus_score(preds, labels).score\n",
    "    print(f\"Test BLEU: {bleu_score:>0.2f}\\n\")\n",
    "    results = []\n",
    "    print(\"saving predicted results...\")\n",
    "    for source, pred, label in zip(sources, preds, labels):\n",
    "        results.append(\n",
    "            {\"sentence\": source, \"prediction\": pred, \"translation\": label[0]}\n",
    "        )\n",
    "    with open(\"test_data_pred.json\", \"wt\", encoding=\"utf-8\") as f:\n",
    "        for exapmle_result in results:\n",
    "            f.write(json.dumps(exapmle_result, ensure_ascii=False) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
