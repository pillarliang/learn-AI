{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")  # load model\n",
    "model.save_pretrained(\"./models/bert-base-cased/\")  # save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a7285fa81c148f7abebbedd2d52f77a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "911c51c5de8441848c209581f4ca321d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "510a1faa28df4cf4aa71922239bba46f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./models/bert-base-cased/tokenizer_config.json',\n",
       " './models/bert-base-cased/special_tokens_map.json',\n",
       " './models/bert-base-cased/vocab.txt',\n",
       " './models/bert-base-cased/added_tokens.json',\n",
       " './models/bert-base-cased/tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # load tokenizer\n",
    "tokenizer.save_pretrained(\"./models/bert-base-cased/\")  # save tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2478, 1037, 10938, 2121, 2897, 2003, 3722, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenizer(\"Using a Transformer network is simple\")\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['using', 'a', 'transform', '##er', 'network', 'is', 'simple']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(\"Using a transformer network is simple\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2478, 1037, 10938, 2121, 2897, 2003, 3722]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2478, 1037, 10938, 2121, 2897, 2003, 3722, 102]\n"
     ]
    }
   ],
   "source": [
    "sequence = tokenizer.encode(\"Using a transformer network is simple\")\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] using a transformer network is simple [SEP]\n"
     ]
    }
   ],
   "source": [
    "original_sequence = tokenizer.decode(\n",
    "    [101, 2478, 1037, 10938, 2121, 2897, 2003, 3722, 102]\n",
    ")\n",
    "print(original_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 处理多段文本\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]\n",
      "tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
      "          2026,  2878,  2166,  1012]])\n",
      "tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "tokens = tokenizer.tokenize(\"I've been waiting for a HuggingFace course my whole life.\")\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "\n",
    "input_ids = torch.tensor([ids])\n",
    "print(input_ids)\n",
    "\n",
    "output = model(input_ids)\n",
    "print(output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "tensor([[-1.5607,  1.6123]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "inputs = tokenizer(\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\", return_tensors=\"pt\"\n",
    ")\n",
    "print(inputs)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# padding - 使得每个 batch 中的 sample 具有相同的长度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "tensor([[-1.5607,  1.6123],\n",
      "        [-3.6183,  3.9137]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(tokens)\n",
    "\n",
    "output = model(**tokens)\n",
    "print(output.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编码句子对\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 2034, 6251, 1012,  102, 2034, 6251, 2003, 2460, 1012,  102,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 2023, 2003, 1996, 2117, 6251, 1012,  102, 1996, 2117, 6251, 2003,\n",
      "         2200, 2200, 2200, 2146, 1012,  102],\n",
      "        [ 101, 2353, 2028, 1012,  102, 7929, 1012,  102,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "torch.Size([3, 18])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sentence1_list = [\"First sentence.\", \"This is the second sentence.\", \"Third one.\"]\n",
    "sentence2_list = [\n",
    "    \"First sentence is short.\",\n",
    "    \"The second sentence is very very very long.\",\n",
    "    \"ok.\",\n",
    "]\n",
    "\n",
    "# 将 sentence1 和 sencentce2 对应索引的句子匹配为句子对。\n",
    "tokens = tokenizer(\n",
    "    sentence1_list, sentence2_list, padding=True, truncation=True, return_tensors=\"pt\"\n",
    ")\n",
    "print(tokens)\n",
    "print(tokens[\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 添加新 token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['two', '[ENT_START]', 'cars', '[ENT_END]', 'collided', 'in', 'a', '[ENT_START]', 'tunnel', '[ENT_END]', 'this', 'morning', '.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "tokenizer.add_tokens([\"[ENT_START]\", \"[ENT_END]\"], special_tokens=True)\n",
    "\n",
    "sentence = \"Two [ENT_START] cars [ENT_END] collided in a [ENT_START] tunnel [ENT_END] this morning.\"\n",
    "print(tokenizer.tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.8909e-03, -1.3111e-02, -9.4551e-02, -5.1809e-02,  3.2491e-03,\n",
      "        -1.2508e-02, -9.8832e-03,  7.4605e-03, -1.5419e-02, -7.5015e-02,\n",
      "        -1.1553e-02, -6.8970e-02, -5.3246e-02,  3.1283e-02, -1.7483e-02,\n",
      "        -1.2914e-02, -3.1466e-02,  7.9941e-05,  1.4641e-02,  1.9323e-02,\n",
      "        -2.6789e-02, -6.0619e-02, -8.2200e-03, -8.3062e-02, -8.5552e-02,\n",
      "        -1.1586e-02,  6.1244e-03,  1.9840e-02, -1.4748e-02, -2.7729e-02,\n",
      "        -2.0458e-02, -6.0993e-03,  6.6157e-03, -4.9288e-03, -7.0220e-02,\n",
      "        -2.7434e-02, -8.8032e-03, -6.3211e-02, -7.4845e-02,  4.3453e-02,\n",
      "         8.4693e-03, -9.0122e-03, -1.1829e-01, -8.4419e-02,  1.2732e-02,\n",
      "        -1.2657e-02, -4.7312e-03, -1.8933e-02, -4.4400e-02, -5.8645e-02,\n",
      "        -9.9826e-03, -1.3900e-02, -5.9908e-02,  3.9422e-03, -5.3567e-02,\n",
      "        -8.7809e-03,  2.7742e-02, -6.1284e-03, -5.7271e-02, -3.9378e-02,\n",
      "        -4.6032e-02, -4.0745e-02, -1.9952e-02,  4.0362e-03, -8.6448e-02,\n",
      "        -7.3612e-02, -2.1270e-02,  9.6670e-03, -1.1053e-02,  3.6903e-02,\n",
      "        -3.3736e-02, -8.4304e-02, -2.0640e-03, -2.8650e-02, -4.1318e-02,\n",
      "        -4.7628e-02, -5.8340e-02,  8.3995e-02, -6.5079e-03,  1.2312e-02,\n",
      "        -1.5833e-02, -2.7054e-02,  2.8281e-02, -8.8524e-03, -4.5735e-02,\n",
      "        -7.7868e-03,  1.5789e-02,  3.2762e-02, -9.3046e-02,  8.2192e-03,\n",
      "        -3.3083e-02, -5.2052e-04, -7.5576e-02,  2.7212e-05, -5.9726e-02,\n",
      "         5.9430e-03,  1.1005e-02, -3.3214e-02, -4.3353e-02,  6.8012e-03,\n",
      "         1.3085e-03,  1.5627e-02,  1.3817e-02, -4.8200e-02, -1.4847e-02,\n",
      "         1.0769e-02, -7.5906e-02, -1.2972e-02, -2.0305e-03, -7.1931e-02,\n",
      "        -7.4099e-02,  3.0527e-02, -4.9369e-02, -7.2289e-02, -6.8892e-02,\n",
      "         1.3263e-02,  1.5259e-02, -2.0756e-02, -6.5433e-02, -5.6889e-02,\n",
      "        -6.8142e-02,  2.6499e-02, -4.9705e-02, -1.5589e-03, -1.0598e-01,\n",
      "        -8.3612e-02, -2.2380e-02, -7.4667e-02,  1.5953e-02, -7.4824e-02,\n",
      "        -1.1363e-03, -2.8911e-02, -2.1411e-02, -7.8669e-02, -2.5889e-02,\n",
      "        -3.0549e-02, -3.4112e-02, -1.7817e-02, -4.8662e-02,  1.7016e-02,\n",
      "        -1.7869e-02,  4.6485e-03, -1.8302e-02, -5.6242e-02, -9.3164e-02,\n",
      "        -1.4210e-02,  1.8625e-02,  2.1137e-02, -4.3141e-02, -3.9653e-02,\n",
      "        -1.1732e-01,  2.7221e-02, -1.2586e-01, -8.2416e-02, -4.2614e-02,\n",
      "        -4.2706e-02, -4.5932e-02, -4.6679e-02, -8.0188e-04,  5.4485e-02,\n",
      "        -4.9422e-02, -3.7727e-03,  2.3172e-02, -4.8600e-03, -4.1012e-02,\n",
      "        -1.5365e-02, -5.1447e-02, -3.8592e-03, -1.1628e-02, -6.7640e-02,\n",
      "        -5.2527e-02,  2.1508e-02, -1.2729e-02, -6.3927e-02, -5.4928e-02,\n",
      "         3.6825e-02, -5.1841e-03, -2.7101e-02, -2.5362e-02,  1.2193e-03,\n",
      "         2.1201e-02,  4.2524e-02, -5.4044e-02,  1.5124e-02, -4.8952e-02,\n",
      "        -2.2192e-02, -8.3094e-02, -3.3215e-03, -2.1449e-02, -6.7673e-02,\n",
      "        -4.1903e-02, -6.5057e-02,  1.2222e-02, -7.1781e-02, -5.8112e-02,\n",
      "        -1.6472e-02, -8.0182e-02, -5.2984e-02, -6.6832e-02, -3.8787e-02,\n",
      "        -5.9649e-03, -5.9950e-04, -4.2840e-03, -3.3441e-02, -8.3544e-02,\n",
      "        -4.4053e-02,  2.1390e-02, -2.8672e-02, -3.4673e-02,  2.4724e-03,\n",
      "        -1.4120e-02,  1.1477e-02, -3.7198e-02, -3.1772e-02, -3.9009e-02,\n",
      "         7.8974e-02,  4.3698e-02,  3.7105e-02, -1.0438e-03, -1.0237e-02,\n",
      "        -2.6285e-02,  3.7143e-02,  1.5000e-02,  9.8743e-03, -4.9844e-02,\n",
      "         1.6737e-02, -5.7224e-02, -1.7933e-02, -8.6965e-03, -5.3894e-02,\n",
      "         5.9402e-03,  1.9277e-02, -8.4592e-02, -1.5567e-03, -3.1843e-02,\n",
      "        -4.1922e-02, -2.3200e-02, -8.2411e-03, -8.2413e-02, -2.5691e-02,\n",
      "         1.2479e-02, -5.8319e-02, -2.9422e-02, -1.7978e-03, -4.3738e-02,\n",
      "         2.8483e-02, -1.5430e-02, -5.3543e-02, -3.4400e-02, -1.4855e-01,\n",
      "        -1.7394e-02, -9.8292e-05,  8.8896e-03,  2.4611e-02,  2.0234e-02,\n",
      "        -3.8682e-02,  9.7463e-05, -1.5845e-01,  2.0571e-02, -5.4073e-03,\n",
      "        -4.6128e-02, -5.2858e-02, -2.5837e-02, -2.9629e-02, -2.7822e-02,\n",
      "        -4.7837e-02, -5.5548e-02, -3.6440e-02, -7.9437e-03, -7.9984e-02,\n",
      "        -9.9150e-02, -1.2495e-02, -4.6067e-02, -1.1527e-02, -4.2295e-02,\n",
      "        -6.0829e-03, -6.7437e-02, -2.2682e-02, -1.1298e-01,  3.3374e-02,\n",
      "         3.3561e-03, -5.3291e-02, -1.7887e-02,  1.5065e-02,  2.5354e-03,\n",
      "        -6.4760e-02,  2.8690e-03,  8.8270e-05, -4.9704e-02, -1.3151e-02,\n",
      "         1.5577e-03,  2.5328e-02, -7.4660e-02,  9.2842e-04, -6.9040e-03,\n",
      "         2.6050e-02, -2.3372e-02, -2.2506e-02, -9.1795e-02,  2.0371e-02,\n",
      "        -8.7559e-02, -4.6209e-02, -3.6459e-02, -6.8412e-02, -9.4616e-02,\n",
      "        -7.5652e-02,  4.7568e-04, -6.9295e-02, -1.6379e-02, -4.6717e-02,\n",
      "        -4.1634e-02,  1.7476e-02, -4.5501e-02, -6.4860e-02, -2.4475e-02,\n",
      "        -5.8141e-02,  1.0870e-03, -5.8295e-04, -6.2581e-02, -1.2693e-02,\n",
      "        -3.9532e-02, -3.0390e-02, -2.1340e-03, -8.6210e-02, -3.3583e-02,\n",
      "        -2.4156e-02, -8.9581e-03,  8.0156e-03, -7.0756e-02, -2.1779e-02,\n",
      "        -4.9724e-02,  4.8395e-02, -4.5818e-02, -5.2647e-02, -6.2059e-02,\n",
      "        -2.6912e-02,  1.2385e-02, -4.4632e-02, -3.1664e-02, -1.5746e-02,\n",
      "        -3.2382e-02,  2.2251e-02, -4.9665e-02, -7.7177e-02, -1.1616e-02,\n",
      "         5.8976e-02, -2.1488e-02, -6.2800e-02, -1.1136e-02, -2.0337e-02,\n",
      "        -6.2497e-02,  3.5125e-02, -2.8682e-02,  6.4651e-02,  2.1420e-03,\n",
      "        -2.6147e-02, -6.3900e-02, -4.0415e-02, -2.5077e-02, -3.1052e-02,\n",
      "        -3.1502e-02, -3.9484e-02,  2.2006e-02, -2.0800e-03, -1.5497e-04,\n",
      "        -1.6408e-02,  2.5685e-03, -3.6224e-03, -1.3886e-02, -4.0504e-03,\n",
      "         1.0812e-02, -1.6268e-02, -1.6173e-03,  2.8297e-02, -3.6881e-02,\n",
      "        -4.2189e-02,  7.9167e-02, -3.7665e-02, -5.7520e-03,  2.1415e-02,\n",
      "         1.9892e-02, -5.1639e-02,  8.7694e-04, -1.0293e-02, -2.2873e-02,\n",
      "        -3.1672e-02,  1.6666e-02, -4.2173e-02, -4.7896e-02, -5.1313e-03,\n",
      "        -4.5635e-02,  2.7137e-03,  1.1827e-02, -5.1285e-02, -1.1196e-02,\n",
      "         3.2842e-02,  1.9806e-02,  3.8356e-02, -7.9122e-03,  4.6462e-02,\n",
      "         2.3951e-02, -6.8637e-02, -6.2127e-02, -8.8952e-02, -1.3375e-02,\n",
      "        -2.9521e-02, -4.2618e-02, -2.8527e-02, -2.2507e-02, -2.5298e-02,\n",
      "        -9.1602e-02,  3.2194e-02, -4.8031e-03, -1.6454e-02, -3.2046e-02,\n",
      "        -2.0042e-02, -7.6906e-02,  4.7528e-03, -5.3851e-02, -4.9252e-02,\n",
      "        -1.4514e-02,  1.6357e-02, -2.3434e-02, -2.0910e-02,  1.2564e-02,\n",
      "        -8.1040e-02, -6.2581e-02, -7.4173e-02, -2.0564e-02, -9.8736e-02,\n",
      "        -1.1010e-02, -7.6014e-02, -3.5629e-02, -2.6374e-02, -2.1334e-02,\n",
      "         1.5477e-02, -1.5002e-04,  3.5708e-02, -1.1575e-02, -4.6454e-02,\n",
      "        -4.7945e-02, -1.2132e-02, -3.7523e-02,  3.4455e-02, -9.1975e-02,\n",
      "        -2.9940e-02, -8.1916e-02, -7.7855e-02, -3.4560e-02, -6.3921e-02,\n",
      "         5.5198e-03, -5.8403e-03, -6.3815e-02, -1.3043e-02, -6.3835e-02,\n",
      "        -2.7477e-02, -8.8213e-02,  1.1199e-02,  9.5303e-03, -1.7543e-03,\n",
      "        -3.0971e-02,  1.6444e-02, -7.2261e-02, -1.3893e-02, -7.6335e-02,\n",
      "        -7.7110e-03, -9.5450e-03,  1.8025e-02, -3.7929e-02, -1.7278e-02,\n",
      "        -2.8929e-03, -4.4193e-02,  1.7456e-02, -7.7897e-02, -3.0623e-02,\n",
      "         9.2248e-02, -4.7609e-02, -9.5240e-02, -6.6848e-03, -1.7129e-02,\n",
      "        -2.8634e-02, -1.2535e-02,  1.0467e-02,  3.2658e-02, -4.1106e-02,\n",
      "        -1.5240e-02, -1.9796e-02, -1.3121e-02,  2.2769e-02, -2.3869e-02,\n",
      "         1.5819e-03,  2.1099e-03, -4.6269e-02, -4.3141e-02, -2.2231e-02,\n",
      "        -4.2840e-02,  5.0938e-03, -3.4719e-02, -4.3223e-03, -1.3358e-02,\n",
      "        -6.6805e-03, -2.5957e-02, -5.0424e-02, -6.8244e-02, -7.1819e-02,\n",
      "        -3.9408e-02, -2.9727e-02, -8.7977e-03, -7.7655e-02, -7.3668e-02,\n",
      "         8.9717e-03, -1.3336e-01, -1.8408e-02, -4.5951e-02, -3.2123e-02,\n",
      "         1.1218e-03,  2.6558e-02,  4.0442e-03,  2.1573e-02, -2.4740e-02,\n",
      "        -7.9003e-03, -4.7497e-02, -2.4379e-02, -1.3333e-02,  4.1764e-03,\n",
      "        -1.7947e-02, -1.7015e-02, -2.6198e-02,  1.0582e-02,  2.7454e-02,\n",
      "        -1.5337e-02, -3.1402e-02, -2.2053e-02, -8.3210e-03,  5.4142e-03,\n",
      "        -5.8168e-02, -2.9439e-02, -6.9745e-02, -1.3607e-01,  1.9239e-02,\n",
      "         1.4422e-02, -9.9482e-03,  3.2948e-02, -6.7280e-02,  3.3705e-02,\n",
      "        -1.2957e-02, -1.9447e-03,  1.1780e-02, -2.5495e-02, -5.1343e-02,\n",
      "        -3.1227e-02, -4.2754e-02,  1.2819e-02, -3.7267e-02, -4.4721e-02,\n",
      "        -5.1889e-02, -1.7465e-02, -3.3361e-02, -3.2884e-02,  4.9383e-03,\n",
      "        -4.0808e-02,  5.3829e-03, -1.0084e-02,  2.6194e-03,  6.9999e-04,\n",
      "        -2.8688e-02, -2.8056e-02, -4.3080e-02, -6.8942e-02, -4.0793e-02,\n",
      "        -4.1335e-02,  3.5789e-04,  2.6966e-02,  2.2749e-02,  1.5422e-02,\n",
      "        -1.3439e-02, -2.2064e-02,  5.0950e-02,  1.4663e-02, -5.1906e-02,\n",
      "        -2.4503e-02, -4.9530e-02, -6.5686e-02,  1.1184e-02,  5.8297e-03,\n",
      "        -4.5430e-02, -3.8321e-02, -5.6829e-02, -2.4786e-02, -1.1486e-01,\n",
      "        -4.7036e-02,  2.4988e-03, -4.1351e-02, -4.1782e-02, -3.4522e-02,\n",
      "         9.7826e-03,  1.4148e-02,  2.5918e-02,  1.3820e-02,  3.3594e-02,\n",
      "        -2.8629e-02,  1.3400e-02,  3.6988e-02, -5.0301e-02,  2.7067e-02,\n",
      "        -3.8333e-02, -4.8890e-02, -8.4943e-02, -4.7772e-02, -2.2766e-02,\n",
      "        -6.3819e-02, -5.5359e-02,  6.5454e-02,  1.8793e-02,  4.0052e-02,\n",
      "        -6.2802e-02, -1.9795e-02,  1.5542e-03, -4.6255e-02, -4.1899e-02,\n",
      "        -4.3590e-02, -2.1529e-02,  1.2136e-02,  5.6100e-03, -5.6544e-03,\n",
      "        -5.0604e-02,  5.4242e-02,  2.2288e-02, -5.7838e-03,  3.2697e-02,\n",
      "        -3.0482e-02, -1.0441e-02, -3.0577e-02, -3.7532e-02,  9.8456e-03,\n",
      "        -8.8160e-03, -1.2852e-02, -5.3600e-02,  1.7507e-03, -5.3459e-02,\n",
      "        -6.2432e-02, -3.4791e-02, -4.0682e-02,  1.4415e-02,  8.0102e-03,\n",
      "        -4.1732e-02, -2.9353e-02, -3.9965e-02, -1.7078e-02,  4.4289e-02,\n",
      "        -4.8901e-02,  7.2780e-04,  2.8571e-02, -1.5023e-02, -2.5579e-02,\n",
      "         1.7369e-02, -2.2370e-02, -7.6134e-02,  4.5081e-03, -4.1430e-03,\n",
      "        -4.6959e-02, -1.9592e-02,  1.0522e-02, -9.4097e-02,  2.0190e-02,\n",
      "        -7.6314e-02,  5.1987e-02, -3.4759e-02, -8.0069e-02, -1.0794e-01,\n",
      "         9.5124e-02, -2.4562e-02, -2.2995e-02, -3.4692e-02, -6.1330e-02,\n",
      "        -2.4257e-02, -6.4656e-02,  4.2841e-02,  5.9090e-03, -2.1947e-03,\n",
      "        -7.4271e-02, -6.3742e-02,  1.9634e-02, -5.6911e-03, -3.2600e-02,\n",
      "        -5.7581e-02, -3.6911e-02, -2.2365e-02, -1.6372e-02,  2.8609e-02,\n",
      "        -2.5164e-02, -2.3653e-02, -4.2006e-02, -2.3248e-02,  5.3002e-02,\n",
      "         1.4676e-02, -8.7402e-03, -4.8626e-02, -3.2395e-02,  5.8032e-02,\n",
      "        -2.4179e-02, -4.3013e-02,  2.5460e-03, -5.9823e-02, -7.1834e-02,\n",
      "        -3.6612e-02, -5.2231e-02,  2.4529e-02, -4.9332e-02, -7.6919e-02,\n",
      "        -4.6393e-02, -1.6269e-02, -3.5184e-02,  3.1396e-02,  1.0654e-02,\n",
      "        -4.1481e-02, -6.1892e-02, -1.7115e-02,  1.0468e-02,  1.9215e-02,\n",
      "         1.7605e-02, -2.5078e-02,  2.4863e-03, -3.2310e-02,  1.2479e-02,\n",
      "        -2.7978e-02, -7.9833e-02,  9.5771e-03, -5.5026e-02, -2.8067e-03,\n",
      "        -6.6178e-02, -5.6192e-02,  3.8935e-03, -2.7744e-02, -1.0533e-01,\n",
      "        -4.8220e-02,  1.1496e-02, -6.8912e-02, -8.7793e-02, -2.0637e-02,\n",
      "        -9.1989e-03, -1.4843e-02,  1.3225e-02,  7.7593e-03, -1.3759e-02,\n",
      "        -2.9858e-02, -4.4050e-03, -2.4263e-02, -3.9404e-02, -6.4698e-03,\n",
      "         2.0144e-03, -1.1284e-01, -3.5497e-02, -5.1476e-02,  1.9550e-03,\n",
      "         9.6221e-03, -5.0076e-02, -2.8153e-02, -1.0288e-01, -4.5045e-02,\n",
      "         1.3602e-02, -3.3344e-02, -6.0530e-02, -6.2490e-02, -1.0856e-02,\n",
      "        -2.2338e-02,  1.0703e-02, -4.1880e-02], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.0016, -0.0289, -0.0082,  ...,  0.0200,  0.0016,  0.0564],\n",
      "        [ 0.0213, -0.0095, -0.0030,  ...,  0.0040, -0.0263, -0.0131]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "# 调整 embedding 矩阵\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# token embedding 初始化 - 初始化为已有 token 的值\n",
    "num_added_tokens = tokenizer.add_tokens(\n",
    "    [\"[ENT_START]\", \"[ENT_END]\"], special_tokens=True\n",
    ")\n",
    "\n",
    "token_id = tokenizer.convert_tokens_to_ids(\"entity\")\n",
    "token_embedding = model.embeddings.word_embeddings.weight[token_id]\n",
    "print(token_embedding)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(1, num_added_tokens + 1):\n",
    "        model.embeddings.word_embeddings.weight[-i:, :] = (\n",
    "            token_embedding.clone().detach().requires_grad_(True)\n",
    "        )  # detach() 这通常用于创建一个不需要梯度计算的张量。\n",
    "        # requires_grad_(True): 这个方法改变了张量的requires_grad属性。如果设置为True，那么在进行梯度计算时，会计算这个张量的梯度。这通常用于训练模型的参数。\n",
    "\n",
    "print(model.embeddings.word_embeddings.weight[-2:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['end', 'of', 'entity']\n",
      "['start', 'of', 'entity']\n",
      "tensor([[-0.0340, -0.0144, -0.0441,  ..., -0.0016,  0.0318, -0.0151],\n",
      "        [-0.0060, -0.0202, -0.0312,  ..., -0.0084,  0.0193, -0.0296]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "description = [\"start of entity\", \"end of entity\"]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, token in enumerate(\n",
    "        reversed(description), start=1\n",
    "    ):  # start=1 表示 i 从1开始计数\n",
    "        tokenized = tokenizer.tokenize(token)\n",
    "        print(tokenized)\n",
    "        tokenized_ids = tokenizer.convert_tokens_to_ids(tokenized)\n",
    "        new_embedding = model.embeddings.word_embeddings.weight[tokenized_ids].mean(\n",
    "            axis=0\n",
    "        )\n",
    "        model.embeddings.word_embeddings.weight[-i, :] = (\n",
    "            new_embedding.clone().detach().requires_grad_(True)\n",
    "        )\n",
    "print(model.embeddings.word_embeddings.weight[-2:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
