{"type": "text", "bbox": [98, 1096, 573, 1426], "res": [{"text": "Thereareseveralaspectsthatareworthdivingdeeper", "confidence": 0.9969345331192017, "text_region": [[126.0, 1099.0], [571.0, 1099.0], [571.0, 1117.0], [126.0, 1117.0]]}, {"text": "into:(1) Overall,the engine is based on rules and human-", "confidence": 0.9593220949172974, "text_region": [[100.0, 1120.0], [571.0, 1121.0], [571.0, 1141.0], [100.0, 1140.0]]}, {"text": "selectedparameters.Theautomationoftheselectionand", "confidence": 0.9988446235656738, "text_region": [[102.0, 1146.0], [571.0, 1146.0], [571.0, 1163.0], [102.0, 1163.0]]}, {"text": "searchfortheseparameterscansavehumaneffortsandhelp", "confidence": 0.9990125298500061, "text_region": [[102.0, 1170.0], [570.0, 1170.0], [570.0, 1188.0], [102.0, 1188.0]]}, {"text": "adapttodifferentscenarios.(2)Whilerenderingsmalltext", "confidence": 0.9934311509132385, "text_region": [[102.0, 1194.0], [571.0, 1194.0], [571.0, 1212.0], [102.0, 1212.0]]}, {"text": "canhelptrainingdetectors,thelowimagequalityof the", "confidence": 0.9887761473655701, "text_region": [[100.0, 1217.0], [571.0, 1216.0], [571.0, 1236.0], [100.0, 1237.0]]}, {"text": "smalltextmakesrecognizersharder totrainandharmsthe", "confidence": 0.9893149733543396, "text_region": [[101.0, 1242.0], [570.0, 1242.0], [570.0, 1260.0], [101.0, 1260.0]]}, {"text": "performance.Designing a method tomarktheillegibleones", "confidence": 0.9793795347213745, "text_region": [[101.0, 1265.0], [571.0, 1265.0], [571.0, 1286.0], [101.0, 1286.0]]}, {"text": "asdifficult andexcluding themfromloss calculationmay", "confidence": 0.9746667146682739, "text_region": [[100.0, 1286.0], [571.0, 1288.0], [571.0, 1309.0], [100.0, 1307.0]]}, {"text": "helpmitigate this problem.(3)For multilingual scene text,", "confidence": 0.9538096189498901, "text_region": [[101.0, 1312.0], [572.0, 1312.0], [572.0, 1332.0], [101.0, 1332.0]]}, {"text": "scriptsexceptLatinhavemuchfeweravailablefontsthatwe", "confidence": 0.9979195594787598, "text_region": [[102.0, 1336.0], [570.0, 1335.0], [570.0, 1354.0], [102.0, 1355.0]]}, {"text": "have easy access to.Toimproveperformance on more lan-", "confidence": 0.9664244055747986, "text_region": [[100.0, 1360.0], [571.0, 1360.0], [571.0, 1381.0], [100.0, 1381.0]]}, {"text": "guages,researchers may considerlearning-based methods", "confidence": 0.9688723683357239, "text_region": [[100.0, 1385.0], [572.0, 1383.0], [572.0, 1403.0], [100.0, 1405.0]]}, {"text": "totransferLatinfontstootherscripts", "confidence": 0.9961358308792114, "text_region": [[100.0, 1407.0], [398.0, 1408.0], [398.0, 1425.0], [100.0, 1425.0]]}], "img_idx": 0}
{"type": "text", "bbox": [616, 421, 1091, 605], "res": [{"text": "Inthispaper,weintroducea scene text image synthesis", "confidence": 0.9631720185279846, "text_region": [[642.0, 421.0], [1089.0, 423.0], [1089.0, 442.0], [642.0, 440.0]]}, {"text": "enginethatrendersimageswith3Dgraphicsengines,where", "confidence": 0.9927992820739746, "text_region": [[619.0, 446.0], [1089.0, 446.0], [1089.0, 465.0], [619.0, 465.0]]}, {"text": "textinstances andscenes are rendered as a whole.Inexper-", "confidence": 0.9628844857215881, "text_region": [[618.0, 467.0], [1087.0, 468.0], [1087.0, 488.0], [618.0, 487.0]]}, {"text": "iments,weverifytheeffectiveness oftheproposedengine", "confidence": 0.9802196621894836, "text_region": [[618.0, 491.0], [1088.0, 492.0], [1088.0, 512.0], [618.0, 511.0]]}, {"text": "inbothscene textdetection andrecognitionmodels.We", "confidence": 0.9808133244514465, "text_region": [[617.0, 515.0], [1089.0, 516.0], [1089.0, 535.0], [617.0, 534.0]]}, {"text": "alsostudykey components of theproposed engine.We be-", "confidence": 0.9543906450271606, "text_region": [[617.0, 539.0], [1087.0, 540.0], [1087.0, 560.0], [617.0, 559.0]]}, {"text": "lieve our workwillbea solid steppingstone towards bette1", "confidence": 0.9459090828895569, "text_region": [[617.0, 563.0], [1087.0, 564.0], [1087.0, 584.0], [617.0, 583.0]]}, {"text": "synthesisalgorithms", "confidence": 0.9978460073471069, "text_region": [[618.0, 588.0], [781.0, 589.0], [781.0, 604.0], [618.0, 604.0]]}], "img_idx": 0}
{"type": "text", "bbox": [98, 667, 573, 994], "res": [{"text": "ExperimentresultsareshowninTab.4.Whenweonlyuse", "confidence": 0.9977005124092102, "text_region": [[101.0, 669.0], [571.0, 671.0], [571.0, 688.0], [101.0, 686.0]]}, {"text": "syntheticdataandcontrolthenumberofimagesto1.2M", "confidence": 0.9982055425643921, "text_region": [[100.0, 695.0], [569.0, 693.0], [569.0, 710.0], [100.0, 712.0]]}, {"text": "oursresult in a considerableimprovement of 1.6%in over", "confidence": 0.9599758386611938, "text_region": [[100.0, 716.0], [569.0, 716.0], [569.0, 736.0], [100.0, 736.0]]}, {"text": "allaccuracy,and significantimprovements onsome scripts", "confidence": 0.977773904800415, "text_region": [[100.0, 740.0], [569.0, 741.0], [569.0, 761.0], [100.0, 760.0]]}, {"text": "e.g.Latin (+7.6%) and Mixed(+21.6%).Using the whole", "confidence": 0.9769619107246399, "text_region": [[99.0, 764.0], [571.0, 761.0], [571.0, 782.0], [99.0, 786.0]]}, {"text": "trainingsetof4.1Mimagesfurtherimprovesoverallaccu-", "confidence": 0.993405818939209, "text_region": [[102.0, 790.0], [570.0, 790.0], [570.0, 807.0], [102.0, 807.0]]}, {"text": "racy to39.5%.Whenwe trainmodels on combinations of", "confidence": 0.9601983428001404, "text_region": [[100.0, 812.0], [572.0, 810.0], [572.0, 831.0], [100.0, 834.0]]}, {"text": "synthetic data and our training split of MLT19,as shown", "confidence": 0.9490158557891846, "text_region": [[99.0, 836.0], [572.0, 834.0], [572.0, 855.0], [99.0, 857.0]]}, {"text": "inthebottomofTab.4,wecanstillobserveaconsiderable", "confidence": 0.995178759098053, "text_region": [[101.0, 861.0], [570.0, 861.0], [570.0, 879.0], [101.0, 879.0]]}, {"text": "marginofourmethodoverSynthTextby3.2%inoverall ac-", "confidence": 0.9833081960678101, "text_region": [[100.0, 884.0], [570.0, 883.0], [570.0, 903.0], [100.0, 904.0]]}, {"text": "curacy.Theexperimentresultsdemonstratethatourmethod", "confidence": 0.9984161257743835, "text_region": [[102.0, 909.0], [571.0, 909.0], [571.0, 927.0], [102.0, 927.0]]}, {"text": "is alsosuperiorinmultilingual scenetextrecognition,and", "confidence": 0.9690279960632324, "text_region": [[100.0, 932.0], [572.0, 932.0], [572.0, 952.0], [100.0, 952.0]]}, {"text": "we believe this resultwill become a stepping stone tofur", "confidence": 0.9463595151901245, "text_region": [[101.0, 956.0], [568.0, 956.0], [568.0, 977.0], [101.0, 977.0]]}, {"text": "therresearch", "confidence": 0.997674286365509, "text_region": [[102.0, 981.0], [204.0, 981.0], [204.0, 993.0], [102.0, 993.0]]}], "img_idx": 0}
{"type": "text", "bbox": [618, 660, 1088, 700], "res": [{"text": "ThisresearchwassupportedbyNationalKeyR&DPro", "confidence": 0.9983044266700745, "text_region": [[644.0, 661.0], [1084.0, 660.0], [1084.0, 677.0], [644.0, 679.0]]}], "img_idx": 0}
{"type": "text", "bbox": [98, 292, 1088, 335], "res": [{"text": "accuracy)", "confidence": 0.9515259861946106, "text_region": [[619.0, 297.0], [694.0, 297.0], [694.0, 307.0], [619.0, 307.0]]}, {"text": "aggregates", "confidence": 0.9968370199203491, "text_region": [[756.0, 296.0], [851.0, 295.0], [851.0, 308.0], [756.0, 310.0]]}, {"text": "English,French,German,anc", "confidence": 0.9320168495178223, "text_region": [[846.0, 293.0], [1087.0, 293.0], [1087.0, 311.0], [846.0, 311.0]]}, {"text": "Italian,asthey", "confidence": 0.98361736536026, "text_region": [[100.0, 316.0], [215.0, 319.0], [214.0, 334.0], [100.0, 331.0]]}, {"text": "afe", "confidence": 0.91034334897995, "text_region": [[222.0, 322.0], [248.0, 322.0], [248.0, 331.0], [222.0, 331.0]]}, {"text": "uatasel", "confidence": 0.8220991492271423, "text_region": [[508.0, 322.0], [559.0, 322.0], [559.0, 331.0], [508.0, 331.0]]}], "img_idx": 0}
{"type": "title", "bbox": [99, 1048, 418, 1066], "res": [{"text": "", "confidence": 0.0, "text_region": [[242.0, 1056.0], [326.0, 1056.0], [326.0, 1060.0], [242.0, 1060.0]]}], "img_idx": 0}
{"type": "title", "bbox": [617, 727, 728, 744], "res": [], "img_idx": 0}
{"type": "title", "bbox": [99, 615, 328, 633], "res": [{"text": "Sxnermentkest", "confidence": 0.7982187271118164, "text_region": [[162.0, 620.0], [316.0, 620.0], [316.0, 628.0], [162.0, 628.0]]}], "img_idx": 0}
{"type": "title", "bbox": [617, 380, 755, 398], "res": [], "img_idx": 0}
{"type": "title", "bbox": [619, 631, 804, 652], "res": [{"text": "Acknowledgemen", "confidence": 0.9701581597328186, "text_region": [[626.0, 636.0], [794.0, 636.0], [794.0, 647.0], [626.0, 647.0]]}], "img_idx": 0}
{"type": "table", "bbox": [97, 375, 587, 523], "res": "", "img_idx": 0}
{"type": "table", "bbox": [129, 138, 1053, 282], "res": "", "img_idx": 0}
{"type": "reference", "bbox": [626, 767, 1089, 1421], "res": [{"text": "[]PabloArbelaez,MichaelMaire,CharlessFowlkes,andJi", "confidence": 0.9721828699111938, "text_region": [[629.0, 770.0], [1086.0, 768.0], [1086.0, 782.0], [629.0, 784.0]]}, {"text": "tendra Malik.Contour detection and hierarchical image seg-", "confidence": 0.9716992378234863, "text_region": [[655.0, 787.0], [1088.0, 790.0], [1088.0, 810.0], [655.0, 808.0]]}, {"text": "mentation.IEEEtransactions onpatternanalysisandma", "confidence": 0.9901202321052551, "text_region": [[657.0, 811.0], [1088.0, 811.0], [1088.0, 831.0], [657.0, 831.0]]}, {"text": "chine intelligence,33(5):898-916,2011.", "confidence": 0.9931216835975647, "text_region": [[657.0, 832.0], [952.0, 832.0], [952.0, 853.0], [657.0, 853.0]]}, {"text": "[2]YoungminBaek,BadoLee,DongyoonHan,SangdooYun", "confidence": 0.9953420162200928, "text_region": [[628.0, 856.0], [1087.0, 856.0], [1087.0, 873.0], [628.0, 873.0]]}, {"text": "andHwalsukLee.Characterregion awarenessfor textdetec", "confidence": 0.987934947013855, "text_region": [[659.0, 878.0], [1085.0, 878.0], [1085.0, 896.0], [659.0, 896.0]]}, {"text": "tion.InProceedingsoftheIEEEConferenceonComputer", "confidence": 0.9873964190483093, "text_region": [[658.0, 900.0], [1087.0, 900.0], [1087.0, 917.0], [658.0, 917.0]]}, {"text": "Vision andPattern Recognition(CVPR),pages 9365-9374", "confidence": 0.9805710911750793, "text_region": [[657.0, 919.0], [1087.0, 920.0], [1087.0, 941.0], [657.0, 940.0]]}, {"text": "2019.", "confidence": 0.9865638017654419, "text_region": [[657.0, 944.0], [697.0, 944.0], [697.0, 959.0], [657.0, 959.0]]}, {"text": "[3]Zhanzhan Cheng,Xuyang Liu,Fan Bai,Yi Niu,Shiliang Pu", "confidence": 0.9633762240409851, "text_region": [[628.0, 967.0], [1086.0, 967.0], [1086.0, 985.0], [628.0, 985.0]]}, {"text": "andShuigengZhou.Arbitrarily-orientedtextrecognition", "confidence": 0.9989327192306519, "text_region": [[658.0, 989.0], [1087.0, 989.0], [1087.0, 1006.0], [658.0, 1006.0]]}, {"text": "CVPR2018,2017.", "confidence": 0.9893182516098022, "text_region": [[659.0, 1010.0], [788.0, 1010.0], [788.0, 1028.0], [659.0, 1028.0]]}, {"text": "[4]CheeKheng Ch'ng and Chee Seng Chan.Total-text:A com", "confidence": 0.9735320806503296, "text_region": [[627.0, 1033.0], [1087.0, 1032.0], [1087.0, 1052.0], [627.0, 1053.0]]}, {"text": "prehensive datasetforscene textdetectionandrecognition", "confidence": 0.9815278053283691, "text_region": [[658.0, 1056.0], [1087.0, 1056.0], [1087.0, 1074.0], [658.0, 1074.0]]}, {"text": "In Proc.ICDAR,volume 1,pages 935-942,2017.", "confidence": 0.9692395329475403, "text_region": [[657.0, 1076.0], [1017.0, 1076.0], [1017.0, 1096.0], [657.0, 1096.0]]}, {"text": "[5]IanGoodfellow,JeanPouget-Abadie,MehdiMirza,Bing", "confidence": 0.9889501333236694, "text_region": [[628.0, 1099.0], [1087.0, 1100.0], [1087.0, 1118.0], [628.0, 1116.0]]}, {"text": "Xu,DavidWarde-Farley,SherjilOzair,AaronCourville,an", "confidence": 0.9700707197189331, "text_region": [[659.0, 1124.0], [1086.0, 1124.0], [1086.0, 1138.0], [659.0, 1138.0]]}, {"text": "YoshuaBengio.Generative adversarialnets.InProc.NIPS", "confidence": 0.9851199984550476, "text_region": [[659.0, 1144.0], [1087.0, 1144.0], [1087.0, 1161.0], [659.0, 1161.0]]}, {"text": "pages 2672-2680, 2014.", "confidence": 0.9890480041503906, "text_region": [[656.0, 1165.0], [833.0, 1161.0], [834.0, 1183.0], [656.0, 1186.0]]}, {"text": "[6]AnkushGupta,AndreaVedaldi,andAndrewZisserman", "confidence": 0.996702253818512, "text_region": [[628.0, 1189.0], [1087.0, 1189.0], [1087.0, 1206.0], [628.0, 1206.0]]}, {"text": "Synthetic data for text localisation in natural images. InProc.", "confidence": 0.9625043272972107, "text_region": [[656.0, 1208.0], [1087.0, 1209.0], [1087.0, 1230.0], [656.0, 1229.0]]}, {"text": "CVPR,pages 23152324,2016.", "confidence": 0.9574609398841858, "text_region": [[659.0, 1233.0], [885.0, 1233.0], [885.0, 1250.0], [659.0, 1250.0]]}, {"text": "[7]Tong He,Zhi Tian,Weilin Huang,Chunhua Shen,Yu Qiao", "confidence": 0.9619812369346619, "text_region": [[628.0, 1256.0], [1086.0, 1256.0], [1086.0, 1274.0], [628.0, 1274.0]]}, {"text": "andChangmingSun.Anend-to-endtextspotterwithexplicit", "confidence": 0.9972381591796875, "text_region": [[658.0, 1278.0], [1087.0, 1278.0], [1087.0, 1295.0], [658.0, 1295.0]]}, {"text": "alignment and attention.In Proc. CVPR,pages 5020-5029", "confidence": 0.9838453531265259, "text_region": [[657.0, 1297.0], [1088.0, 1297.0], [1088.0, 1318.0], [657.0, 1318.0]]}, {"text": "2018.", "confidence": 0.9805755615234375, "text_region": [[657.0, 1321.0], [697.0, 1321.0], [697.0, 1336.0], [657.0, 1336.0]]}, {"text": "[8]StefanHinterstoisser, OlivierPauly,Hauke Heibel,Martina", "confidence": 0.971477746963501, "text_region": [[628.0, 1342.0], [1086.0, 1342.0], [1086.0, 1360.0], [628.0, 1360.0]]}, {"text": "Marek,andMartinBokeloh.An annotation saved is an an-", "confidence": 0.9610022902488708, "text_region": [[658.0, 1366.0], [1087.0, 1366.0], [1087.0, 1383.0], [658.0, 1383.0]]}, {"text": "notation earned:Using fully synthetic training for object in-", "confidence": 0.9779661893844604, "text_region": [[656.0, 1385.0], [1088.0, 1386.0], [1088.0, 1407.0], [656.0, 1406.0]]}, {"text": "stancedetection.\uff08oRRabs/1902.099672019", "confidence": 0.9451590180397034, "text_region": [[659.0, 1411.0], [993.0, 1411.0], [993.0, 1420.0], [659.0, 1420.0]]}], "img_idx": 0}
{"type": "reference", "bbox": [101, 533, 572, 555], "res": [{"text": "able5:ResultsonEnglishda", "confidence": 0.8786733746528625, "text_region": [[110.0, 536.0], [342.0, 536.0], [342.0, 545.0], [110.0, 545.0]]}, {"text": "asets(wordlevelaccuracy", "confidence": 0.9481464624404907, "text_region": [[348.0, 536.0], [556.0, 537.0], [556.0, 547.0], [348.0, 546.0]]}], "img_idx": 0}
