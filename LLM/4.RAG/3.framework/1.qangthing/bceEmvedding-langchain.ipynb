{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting BCEmbedding==0.1.3\n",
      "  Downloading BCEmbedding-0.1.3-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: torch>=1.6.0 in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from BCEmbedding==0.1.3) (2.2.1)\n",
      "Requirement already satisfied: transformers>=4.35.0 in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from BCEmbedding==0.1.3) (4.40.0.dev0)\n",
      "Requirement already satisfied: datasets in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from BCEmbedding==0.1.3) (2.18.0)\n",
      "Collecting sentence-transformers (from BCEmbedding==0.1.3)\n",
      "  Using cached sentence_transformers-2.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: filelock in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from torch>=1.6.0->BCEmbedding==0.1.3) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from torch>=1.6.0->BCEmbedding==0.1.3) (4.10.0)\n",
      "Requirement already satisfied: sympy in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from torch>=1.6.0->BCEmbedding==0.1.3) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from torch>=1.6.0->BCEmbedding==0.1.3) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from torch>=1.6.0->BCEmbedding==0.1.3) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from torch>=1.6.0->BCEmbedding==0.1.3) (2024.2.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from transformers>=4.35.0->BCEmbedding==0.1.3) (0.21.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from transformers>=4.35.0->BCEmbedding==0.1.3) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from transformers>=4.35.0->BCEmbedding==0.1.3) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from transformers>=4.35.0->BCEmbedding==0.1.3) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from transformers>=4.35.0->BCEmbedding==0.1.3) (2023.12.25)\n",
      "Requirement already satisfied: requests in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from transformers>=4.35.0->BCEmbedding==0.1.3) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from transformers>=4.35.0->BCEmbedding==0.1.3) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from transformers>=4.35.0->BCEmbedding==0.1.3) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from transformers>=4.35.0->BCEmbedding==0.1.3) (4.66.2)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from datasets->BCEmbedding==0.1.3) (15.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from datasets->BCEmbedding==0.1.3) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from datasets->BCEmbedding==0.1.3) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from datasets->BCEmbedding==0.1.3) (2.2.1)\n",
      "Requirement already satisfied: xxhash in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from datasets->BCEmbedding==0.1.3) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from datasets->BCEmbedding==0.1.3) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from datasets->BCEmbedding==0.1.3) (3.9.3)\n",
      "Requirement already satisfied: scikit-learn in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from sentence-transformers->BCEmbedding==0.1.3) (1.4.1.post1)\n",
      "Requirement already satisfied: scipy in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from sentence-transformers->BCEmbedding==0.1.3) (1.12.0)\n",
      "Requirement already satisfied: Pillow in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from sentence-transformers->BCEmbedding==0.1.3) (10.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from aiohttp->datasets->BCEmbedding==0.1.3) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from aiohttp->datasets->BCEmbedding==0.1.3) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from aiohttp->datasets->BCEmbedding==0.1.3) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from aiohttp->datasets->BCEmbedding==0.1.3) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from aiohttp->datasets->BCEmbedding==0.1.3) (1.9.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from requests->transformers>=4.35.0->BCEmbedding==0.1.3) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from requests->transformers>=4.35.0->BCEmbedding==0.1.3) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from requests->transformers>=4.35.0->BCEmbedding==0.1.3) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from requests->transformers>=4.35.0->BCEmbedding==0.1.3) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from jinja2->torch>=1.6.0->BCEmbedding==0.1.3) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from pandas->datasets->BCEmbedding==0.1.3) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from pandas->datasets->BCEmbedding==0.1.3) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from pandas->datasets->BCEmbedding==0.1.3) (2024.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from scikit-learn->sentence-transformers->BCEmbedding==0.1.3) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from scikit-learn->sentence-transformers->BCEmbedding==0.1.3) (3.3.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from sympy->torch>=1.6.0->BCEmbedding==0.1.3) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/liangzhu/anaconda3/envs/llm/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets->BCEmbedding==0.1.3) (1.16.0)\n",
      "Downloading BCEmbedding-0.1.3-py3-none-any.whl (29 kB)\n",
      "Using cached sentence_transformers-2.6.1-py3-none-any.whl (163 kB)\n",
      "Installing collected packages: sentence-transformers, BCEmbedding\n",
      "Successfully installed BCEmbedding-0.1.3 sentence-transformers-2.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install BCEmbedding==0.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/08/2024 14:53:47 - [INFO] -sentence_transformers.SentenceTransformer->>>    Load pretrained SentenceTransformer: maidalun1020/bce-embedding-base_v1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fab86b6d4984eda9753c5a628cf8e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0663db61b3324fb586a09fc22277b2b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/117 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83b2d832216544c8950b76ba15d3e479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/36.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d750529580547cdb87f361af50c6ee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f80e75dc7649818ce131eb5f654773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/741 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ac42a55a624858aa0c98f3a947ae1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f12f4e3bd7c4d9aaa3be8f30049f89f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c496863ba9f64263aece9b88358e5580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eecdd8139ad74f8b8704c5cd823043a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0650be25eea4a268e5f22bc13c911a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd7fc36dfde14cefa9fa584d3a24635e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m embedding_model_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m     16\u001b[0m embedding_encode_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m32\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormalize_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshow_progress_bar\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n\u001b[0;32m---> 18\u001b[0m embed_model \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceEmbeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_model_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m  \u001b[49m\u001b[43mencode_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_encode_kwargs\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m reranker_args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaidalun1020/bce-reranker-base_v1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop_n\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m5\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:1\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m     25\u001b[0m reranker \u001b[38;5;241m=\u001b[39m BCERerank(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mreranker_args)\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/langchain_community/embeddings/huggingface.py:67\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import sentence_transformers python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install sentence-transformers`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m \u001b[43msentence_transformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:215\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, cache_folder, trust_remote_code, revision, token, use_auth_token)\u001b[0m\n\u001b[1;32m    212\u001b[0m     device \u001b[38;5;241m=\u001b[39m get_device_name()\n\u001b[1;32m    213\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse pytorch device_name: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(device))\n\u001b[0;32m--> 215\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_prompt_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_prompt_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompts:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefault prompt name \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_prompt_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found in the configured prompts \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdictionary with keys \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompts\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 802 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/torch/cuda/__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m     )\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# 我们在`BCEmbedding`中提供langchain直接集成的接口。\n",
    "from BCEmbedding.tools.langchain import BCERerank\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "\n",
    "\n",
    "# init embedding model\n",
    "embedding_model_name = 'maidalun1020/bce-embedding-base_v1'\n",
    "# embedding_model_kwargs = {'device': 'cuda:0'}\n",
    "embedding_encode_kwargs = {'batch_size': 32, 'normalize_embeddings': True, 'show_progress_bar': False}\n",
    "\n",
    "embed_model = HuggingFaceEmbeddings(\n",
    "  model_name=embedding_model_name,\n",
    "#   model_kwargs=embedding_model_kwargs,\n",
    "  encode_kwargs=embedding_encode_kwargs\n",
    ")\n",
    "\n",
    "reranker_args = {'model': 'maidalun1020/bce-reranker-base_v1', 'top_n': 5, 'device': 'cuda:1'}\n",
    "reranker = BCERerank(**reranker_args)\n",
    "\n",
    "# init documents\n",
    "documents = PyPDFLoader(\"./data/高速公路养护管理办法.pdf\").load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# example 1. retrieval with embedding and reranker\n",
    "retriever = FAISS.from_documents(texts, embed_model, distance_strategy=DistanceStrategy.MAX_INNER_PRODUCT).as_retriever(search_type=\"similarity\", search_kwargs={\"score_threshold\": 0.3, \"k\": 10})\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=reranker, base_retriever=retriever\n",
    ")\n",
    "\n",
    "response = compression_retriever.get_relevant_documents(\"What is Llama 2?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
