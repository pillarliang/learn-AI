{"type": "text", "bbox": [98, 398, 573, 912], "res": [{"text": "Duringtheexperiments ofscenetextrecognitionforEn-", "confidence": 0.9895027279853821, "text_region": [[125.0, 398.0], [570.0, 398.0], [570.0, 418.0], [125.0, 418.0]]}, {"text": "glish scripts,we notice that among the most widelyused", "confidence": 0.9452914595603943, "text_region": [[102.0, 421.0], [572.0, 421.0], [572.0, 441.0], [102.0, 441.0]]}, {"text": "benchmarkdatasets,severalhaveincompleteannotations", "confidence": 0.990002453327179, "text_region": [[102.0, 446.0], [568.0, 446.0], [568.0, 463.0], [102.0, 463.0]]}, {"text": "They areIIIT5K,SVT,SVTP,and CUTE-80.The annota-", "confidence": 0.9480559825897217, "text_region": [[100.0, 467.0], [571.0, 468.0], [571.0, 488.0], [100.0, 487.0]]}, {"text": "tions of these datasets are case-insensitive,and ignore punc", "confidence": 0.9548248648643494, "text_region": [[99.0, 491.0], [569.0, 493.0], [569.0, 513.0], [99.0, 511.0]]}, {"text": "tuationmarks.", "confidence": 0.9780445694923401, "text_region": [[103.0, 519.0], [215.0, 519.0], [215.0, 534.0], [103.0, 534.0]]}, {"text": "Thecommonpracticeforrecentscenetextrecognition", "confidence": 0.9992943406105042, "text_region": [[123.0, 540.0], [571.0, 541.0], [571.0, 561.0], [123.0, 560.0]]}, {"text": "researchistoconvertbothpredictionandground-truthtext", "confidence": 0.9979109168052673, "text_region": [[101.0, 566.0], [571.0, 565.0], [571.0, 582.0], [101.0, 583.0]]}, {"text": "stringstolower-caseandthencomparethem.Thismeans", "confidence": 0.9988812208175659, "text_region": [[101.0, 590.0], [571.0, 590.0], [571.0, 607.0], [101.0, 607.0]]}, {"text": "that the current evaluationisflawed.Itignoresletter case", "confidence": 0.9715712666511536, "text_region": [[99.0, 611.0], [572.0, 613.0], [572.0, 633.0], [99.0, 631.0]]}, {"text": "andpunctuationmarkswhicharecrucialtotheunderstand", "confidence": 0.9984733462333679, "text_region": [[102.0, 637.0], [569.0, 637.0], [569.0, 654.0], [102.0, 654.0]]}, {"text": "ingofthetextcontents.Besides,evaluatingonamuch", "confidence": 0.9907326698303223, "text_region": [[102.0, 661.0], [571.0, 661.0], [571.0, 678.0], [102.0, 678.0]]}, {"text": "smallervocabularysetresultsin over-optimism of theper-", "confidence": 0.974994421005249, "text_region": [[100.0, 682.0], [571.0, 684.0], [571.0, 704.0], [100.0, 702.0]]}, {"text": "formanceoftherecognitionmodels.", "confidence": 0.9847807884216309, "text_region": [[102.0, 709.0], [387.0, 709.0], [387.0, 726.0], [102.0, 726.0]]}, {"text": "Toaid further research,we use the Amazon mechan-", "confidence": 0.9517845511436462, "text_region": [[123.0, 730.0], [570.0, 731.0], [570.0, 751.0], [123.0, 750.0]]}, {"text": "icalTurk(AMT\uff09tore-annotate theaforementioned4", "confidence": 0.9756576418876648, "text_region": [[101.0, 755.0], [572.0, 755.0], [572.0, 775.0], [101.0, 775.0]]}, {"text": "latasets,whichamountto6837wordimagesintotal", "confidence": 0.9830257296562195, "text_region": [[104.0, 782.0], [568.0, 782.0], [568.0, 797.0], [104.0, 797.0]]}, {"text": "Eachwordimageisannotatedby3workers,andwe", "confidence": 0.9880527257919312, "text_region": [[101.0, 804.0], [571.0, 805.0], [571.0, 822.0], [101.0, 821.0]]}, {"text": "manuallycheckandcorrectimageswherethe3an-", "confidence": 0.9905795454978943, "text_region": [[102.0, 829.0], [570.0, 829.0], [570.0, 846.0], [102.0, 846.0]]}, {"text": "notationsdiffer.", "confidence": 0.9976100325584412, "text_region": [[102.0, 854.0], [241.0, 854.0], [241.0, 868.0], [102.0, 868.0]]}, {"text": "Theannotateddatasetsarereleasec", "confidence": 0.9825746417045593, "text_region": [[261.0, 854.0], [569.0, 854.0], [569.0, 868.0], [261.0, 868.0]]}, {"text": "viaGitHubathttps://github.com/Jyouhou/", "confidence": 0.9948514699935913, "text_region": [[100.0, 876.0], [567.0, 877.0], [567.0, 894.0], [100.0, 893.0]]}], "img_idx": 0}
{"type": "text", "bbox": [99, 1075, 574, 1331], "res": [{"text": "As we areencouragingcase-sensitive(alsowith punctua-", "confidence": 0.974373459815979, "text_region": [[124.0, 1075.0], [571.0, 1076.0], [571.0, 1096.0], [124.0, 1095.0]]}, {"text": "tionmarks)evaluationfor scenetextrecognition,wewould", "confidence": 0.9878533482551575, "text_region": [[101.0, 1099.0], [572.0, 1098.0], [572.0, 1118.0], [101.0, 1119.0]]}, {"text": "liketoprovidebenchmarkperformancesonthosewidely", "confidence": 0.9990656971931458, "text_region": [[101.0, 1123.0], [571.0, 1123.0], [571.0, 1143.0], [101.0, 1143.0]]}, {"text": "useddatasets.Weevaluatetwoimplementationsof the", "confidence": 0.9882621169090271, "text_region": [[100.0, 1147.0], [572.0, 1146.0], [572.0, 1166.0], [100.0, 1167.0]]}, {"text": "ASTER models,byLong et al.'andBaeket alrespec-", "confidence": 0.935720682144165, "text_region": [[101.0, 1170.0], [570.0, 1170.0], [570.0, 1190.0], [101.0, 1190.0]]}, {"text": "tively.Results are summarized in Tab.7.", "confidence": 0.9671229720115662, "text_region": [[100.0, 1196.0], [427.0, 1193.0], [428.0, 1213.0], [100.0, 1216.0]]}, {"text": "Thetwobenchmarkimplementationsperformcompara-", "confidence": 0.9972662329673767, "text_region": [[124.0, 1218.0], [572.0, 1219.0], [572.0, 1239.0], [124.0, 1238.0]]}, {"text": "bly,withBaek'sbetter on straighttext andLong'sbetter at", "confidence": 0.9591116905212402, "text_region": [[101.0, 1242.0], [573.0, 1243.0], [573.0, 1263.0], [101.0, 1262.0]]}, {"text": "curvedtext.Comparedwithevaluationwithlowercase+", "confidence": 0.998347818851471, "text_region": [[102.0, 1268.0], [572.0, 1268.0], [572.0, 1285.0], [102.0, 1285.0]]}, {"text": "digits,theperformancedropsconsiderablyforbothmodels", "confidence": 0.9930998086929321, "text_region": [[101.0, 1292.0], [571.0, 1290.0], [571.0, 1309.0], [101.0, 1311.0]]}, {"text": "whenweevaluatewithallsymbols.Theseresultsindicate", "confidence": 0.996159017086029, "text_region": [[100.0, 1315.0], [571.0, 1314.0], [571.0, 1330.0], [100.0, 1330.0]]}], "img_idx": 0}
{"type": "text", "bbox": [98, 187, 573, 297], "res": [{"text": "Inthiswork,weuseatotalnumberof30scenemodels", "confidence": 0.9812684059143066, "text_region": [[126.0, 189.0], [570.0, 189.0], [570.0, 205.0], [126.0, 205.0]]}, {"text": "whichareallobtainedfromtheInternet.However,mostof", "confidence": 0.9867541193962097, "text_region": [[102.0, 212.0], [570.0, 212.0], [570.0, 228.0], [102.0, 228.0]]}, {"text": "thesemodels arenotfree.Therefore,we arenot allowed to", "confidence": 0.9768580794334412, "text_region": [[100.0, 234.0], [571.0, 235.0], [571.0, 255.0], [100.0, 253.0]]}, {"text": "sharethemodelsthemselves.Instead,welistthemodelswe", "confidence": 0.9863555431365967, "text_region": [[102.0, 260.0], [570.0, 260.0], [570.0, 276.0], [102.0, 276.0]]}], "img_idx": 0}
{"type": "text", "bbox": [616, 502, 1087, 542], "res": [], "img_idx": 0}
{"type": "text", "bbox": [100, 976, 571, 1017], "res": [{"text": "stratethenew", "confidence": 0.9971721172332764, "text_region": [[100.0, 1000.0], [214.0, 1002.0], [214.0, 1016.0], [100.0, 1014.0]]}, {"text": "annotations", "confidence": 0.9960553646087646, "text_region": [[214.0, 1003.0], [308.0, 1002.0], [308.0, 1014.0], [214.0, 1016.0]]}, {"text": "inFig.", "confidence": 0.8431763052940369, "text_region": [[310.0, 1002.0], [363.0, 1002.0], [363.0, 1014.0], [310.0, 1014.0]]}], "img_idx": 0}
{"type": "title", "bbox": [99, 327, 571, 375], "res": [{"text": "B.NewAnnotationsforScene", "confidence": 0.9967624545097351, "text_region": [[102.0, 329.0], [422.0, 334.0], [421.0, 350.0], [102.0, 345.0]]}, {"text": "TextRecogni", "confidence": 0.9937676787376404, "text_region": [[418.0, 333.0], [562.0, 331.0], [562.0, 348.0], [419.0, 350.0]]}, {"text": "tionDatasets", "confidence": 0.9974325299263, "text_region": [[100.0, 355.0], [233.0, 355.0], [233.0, 374.0], [100.0, 374.0]]}], "img_idx": 0}
{"type": "title", "bbox": [100, 1037, 380, 1054], "res": [{"text": "2n31k28", "confidence": 0.22682391107082367, "text_region": [[175.0, 1043.0], [280.0, 1043.0], [280.0, 1049.0], [175.0, 1049.0]]}], "img_idx": 0}
{"type": "title", "bbox": [100, 939, 214, 958], "res": [{"text": "B.1Sample", "confidence": 0.9933350682258606, "text_region": [[105.0, 939.0], [204.0, 941.0], [204.0, 955.0], [104.0, 952.0]]}], "img_idx": 0}
{"type": "title", "bbox": [99, 147, 268, 165], "res": [{"text": "SceneMode", "confidence": 0.9895869493484497, "text_region": [[130.0, 150.0], [256.0, 150.0], [256.0, 162.0], [130.0, 162.0]]}], "img_idx": 0}
{"type": "figure", "bbox": [611, 143, 1091, 419], "res": [{"text": "Dataset", "confidence": 0.9972609877586365, "text_region": [[621.0, 151.0], [672.0, 151.0], [672.0, 166.0], [621.0, 166.0]]}, {"text": "SampleImage", "confidence": 0.9952896237373352, "text_region": [[701.0, 148.0], [794.0, 151.0], [794.0, 168.0], [701.0, 165.0]]}, {"text": "Original Annotation", "confidence": 0.9785372018814087, "text_region": [[825.0, 150.0], [954.0, 150.0], [954.0, 166.0], [825.0, 166.0]]}, {"text": "NewAnnotation", "confidence": 0.9985300898551941, "text_region": [[984.0, 151.0], [1086.0, 151.0], [1086.0, 164.0], [984.0, 164.0]]}, {"text": "Team", "confidence": 0.9889776110649109, "text_region": [[716.0, 178.0], [786.0, 195.0], [780.0, 216.0], [710.0, 199.0]]}, {"text": "CUTE80", "confidence": 0.9984726905822754, "text_region": [[620.0, 190.0], [678.0, 190.0], [678.0, 206.0], [620.0, 206.0]]}, {"text": "TEAM", "confidence": 0.9993855357170105, "text_region": [[860.0, 191.0], [903.0, 191.0], [903.0, 205.0], [860.0, 205.0]]}, {"text": "Team", "confidence": 0.9956225156784058, "text_region": [[1018.0, 190.0], [1054.0, 192.0], [1053.0, 207.0], [1017.0, 205.0]]}, {"text": "15%.", "confidence": 0.9930968284606934, "text_region": [[709.0, 240.0], [788.0, 236.0], [790.0, 273.0], [711.0, 277.0]]}, {"text": "IIIT5K", "confidence": 0.9603788256645203, "text_region": [[621.0, 253.0], [664.0, 253.0], [664.0, 268.0], [621.0, 268.0]]}, {"text": "15", "confidence": 0.9994914531707764, "text_region": [[872.0, 252.0], [891.0, 252.0], [891.0, 269.0], [872.0, 269.0]]}, {"text": "15%.", "confidence": 0.8868655562400818, "text_region": [[1019.0, 252.0], [1050.0, 252.0], [1050.0, 268.0], [1019.0, 268.0]]}, {"text": "SVT", "confidence": 0.9987900257110596, "text_region": [[620.0, 315.0], [653.0, 315.0], [653.0, 332.0], [620.0, 332.0]]}, {"text": "Donald", "confidence": 0.9894009232521057, "text_region": [[709.0, 310.0], [788.0, 314.0], [787.0, 338.0], [708.0, 334.0]]}, {"text": "DONALD", "confidence": 0.9997202754020691, "text_region": [[848.0, 315.0], [914.0, 315.0], [914.0, 331.0], [848.0, 331.0]]}, {"text": "Donald'", "confidence": 0.9564723372459412, "text_region": [[1010.0, 315.0], [1060.0, 315.0], [1060.0, 332.0], [1010.0, 332.0]]}, {"text": "SVTP", "confidence": 0.9980090260505676, "text_region": [[621.0, 378.0], [660.0, 378.0], [660.0, 392.0], [621.0, 392.0]]}, {"text": "MARLBORO", "confidence": 0.9991759657859802, "text_region": [[839.0, 379.0], [924.0, 379.0], [924.0, 392.0], [839.0, 392.0]]}, {"text": "Marlboro", "confidence": 0.9945895075798035, "text_region": [[1005.0, 375.0], [1065.0, 377.0], [1065.0, 394.0], [1005.0, 392.0]]}], "img_idx": 0}
{"type": "figure_caption", "bbox": [683, 435, 1023, 453], "res": [{"text": "?", "confidence": 0.05598534643650055, "text_region": [[702.0, 442.0], [733.0, 442.0], [733.0, 446.0], [702.0, 446.0]]}, {"text": "", "confidence": 0.0, "text_region": [[830.0, 442.0], [918.0, 442.0], [918.0, 446.0], [830.0, 446.0]]}], "img_idx": 0}
{"type": "reference", "bbox": [101, 1352, 416, 1426], "res": [{"text": "https://qithub.com/Jyouhou/", "confidence": 0.9886592626571655, "text_region": [[128.0, 1352.0], [384.0, 1352.0], [384.0, 1367.0], [128.0, 1367.0]]}, {"text": "ICDAR2019-ArT-Recognition-Alchemy", "confidence": 0.9910761713981628, "text_region": [[102.0, 1372.0], [415.0, 1372.0], [415.0, 1388.0], [102.0, 1388.0]]}, {"text": "benchmark", "confidence": 0.9982344508171082, "text_region": [[313.0, 1412.0], [398.0, 1412.0], [398.0, 1424.0], [313.0, 1424.0]]}], "img_idx": 0}
