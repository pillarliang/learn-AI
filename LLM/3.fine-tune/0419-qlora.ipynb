{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "vllm 0.4.0.post1 requires torch==2.1.2, but you have torch 2.2.2 which is incompatible.\n",
      "bcembedding 0.1.4 requires transformers<4.37.0,>=4.35.0, but you have transformers 4.40.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -U bitsandbytes transformers peft accelerate datasets scipy einops evaluate trl rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6,7\"\n",
    "\n",
    "os.environ['http_proxy'] = 'http://127.0.0.1:1080'\n",
    "os.environ['https_proxy'] = 'http://127.0.0.1:1080'\n",
    "os.environ['ftp_proxy'] = 'http://127.0.0.1:1080'\n",
    "os.environ['no_proxy'] = '172.16.x.x,127.0.0.1,localhost'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved in your configured git credential helpers (store).\n",
      "Your token has been saved to /home/liangzhu/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import interpreter_login\n",
    "\n",
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied:{info.used // 1024**2} MB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50fbb091d7334a24b7624b0cea1f723b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/4.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.81M/1.81M [00:01<00:00, 1.71MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 441k/441k [00:00<00:00, 533kB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 447k/447k [00:00<00:00, 637kB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5975f06a31e491792b01352d99a6dd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b2ba8c5c6174498885737387e724be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4f415ed91594f73bbcbe41d73e59362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'train_0',\n",
       " 'dialogue': \"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\",\n",
       " 'summary': \"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\",\n",
       " 'topic': 'get a check-up'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "huggingface_data_name = 'neil-code/dialogsum-test'\n",
    "dataset = load_dataset(huggingface_data_name)\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataset\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load model -- quantization\n",
    "\n",
    "##### BitsAndBytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:08<00:00,  4.23s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type = 'nf4', # using BitsAndBytesConfig to load our model in 4-bit format.\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "device_map = {\"\": 0} # è¡¨ç¤ºå°†æ¨¡åž‹çš„æ‰€æœ‰éƒ¨åˆ†ï¼ˆå› ä¸ºé”®æ˜¯ç©ºå­—ç¬¦ä¸²ï¼Œè¿™é€šå¸¸æ„å‘³ç€é»˜è®¤æˆ–å…¨å±€è®¾ç½®ï¼‰æ”¾ç½®åœ¨è®¾å¤‡ 6 ä¸Šã€‚\n",
    "model_name = 'microsoft/phi-2'\n",
    "original_model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                      device_map=device_map,\n",
    "                                                      quantization_config=bnb_config,\n",
    "                                                      trust_remote_code=True,\n",
    "                                                      use_auth_token=True,\n",
    "                                                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, \n",
    "                                          trust_remote_code=True, \n",
    "                                          padding_side='left', \n",
    "                                          add_eos_token=True,\n",
    "                                          add_bos_token=True,\n",
    "                                          use_fast=False,)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**model_name:**\n",
    "\n",
    "- ç”¨é€”: æŒ‡å®šé¢„è®­ç»ƒæ¨¡åž‹çš„åç§°æˆ–è·¯å¾„ã€‚è¿™å¯ä»¥æ˜¯æœ¬åœ°è·¯å¾„æˆ–è€…æ˜¯ Hugging Face æ¨¡åž‹ä»“åº“ä¸­çš„æ¨¡åž‹åç§°ã€‚\n",
    "- ç¤ºä¾‹: \"bert-base-uncased\", \"gpt2\", æˆ–è€…ä¸€ä¸ªæœ¬åœ°æ–‡ä»¶å¤¹è·¯å¾„ã€‚\n",
    "\n",
    "**trust_remote_code (é»˜è®¤ä¸º False):**\n",
    "\n",
    "- è¯´æ˜Žï¼šå½“ä½ ä»Ž Hugging Face æ¨¡åž‹åº“ä¸‹è½½ä¸€ä¸ªæ¨¡åž‹æ—¶ï¼Œè¯¥åº“ä¸ä»…åŒ…å«æ¨¡åž‹çš„æƒé‡å’Œé…ç½®æ–‡ä»¶ï¼Œè¿˜å¯èƒ½åŒ…å«ä¸€äº› Python è„šæœ¬ï¼Œè¿™äº›è„šæœ¬å®šä¹‰äº†å¦‚ä½•æ­£ç¡®åœ°å¤„ç†ã€æ ‡è®°åŒ–æˆ–è€…é¢„å¤„ç†æ•°æ®ä»¥é€‚åº”è¯¥æ¨¡åž‹ã€‚å¦‚æžœ trust_remote_code è®¾ç½®ä¸º Trueï¼Œè¿™äº›è„šæœ¬å°†è¢«åŠ è½½å¹¶æ‰§è¡Œã€‚\n",
    "- ç”¨é€”: è¿™ä¸ªå‚æ•°å…è®¸åŠ è½½è¿œç¨‹ä»“åº“ä¸­çš„è‡ªå®šä¹‰ä»£ç ã€‚å½“è®¾ç½®ä¸º True æ—¶ï¼Œå¦‚æžœè¿œç¨‹æ¨¡åž‹åŒ…å«è‡ªå®šä¹‰çš„æ ‡è®°åŒ–å™¨ä»£ç ï¼Œè¿™äº›ä»£ç å°†è¢«åŠ è½½å¹¶æ‰§è¡Œã€‚\n",
    "  å®‰å…¨è€ƒè™‘: è®¾ç½®ä¸º True å¯èƒ½ä¼šå¸¦æ¥å®‰å…¨é£Žé™©ï¼Œå› ä¸ºæ‰§è¡Œè¿œç¨‹ä»£ç å¯èƒ½ä¼šè¿è¡Œæ¶æ„è„šæœ¬ã€‚åªåœ¨ä½ ä¿¡ä»»è¿œç¨‹æºçš„æƒ…å†µä¸‹ä½¿ç”¨ã€‚\n",
    "\n",
    "**padding_side (é»˜è®¤ä¸º \"right\"):**\n",
    "\n",
    "https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa\n",
    "\n",
    "- ç”¨é€”: è®¾ç½®æ ‡è®°åŒ–åŽçš„åºåˆ—å¡«å……æ–¹å‘ã€‚å¯ä»¥è®¾ç½®ä¸º \"left\" æˆ– \"right\"ã€‚\n",
    "- ç¤ºä¾‹: å¦‚æžœè®¾ç½®ä¸º \"left\"ï¼Œå¡«å……å°†ä¼šæ·»åŠ åˆ°åºåˆ—çš„å·¦ä¾§ï¼Œå¦‚åœ¨å¤„ç†å³å¯¹é½æ–‡æœ¬æ—¶å¯èƒ½éœ€è¦è¿™æ ·åšã€‚\n",
    "\n",
    "**add_eos_token (ä¸€èˆ¬ä¸ºæ¨¡åž‹ç‰¹å®šçš„é»˜è®¤å€¼):**\n",
    "\n",
    "- ç”¨é€”: å†³å®šæ˜¯å¦åœ¨åºåˆ—çš„æœ«å°¾è‡ªåŠ¨æ·»åŠ ç»ˆæ­¢ç¬¦ï¼ˆEOSï¼Œå³ End Of Sentenceï¼‰æ ‡è®°ã€‚\n",
    "- æ³¨æ„: å¹¶éžæ‰€æœ‰æ¨¡åž‹éƒ½ä½¿ç”¨ EOS æ ‡è®°ï¼Œè¿™ä¾èµ–äºŽç‰¹å®šæ¨¡åž‹çš„é¢„è®­ç»ƒè¿‡ç¨‹ã€‚\n",
    "\n",
    "**add_bos_token (ä¸€èˆ¬ä¸ºæ¨¡åž‹ç‰¹å®šçš„é»˜è®¤å€¼):**\n",
    "\n",
    "- ç”¨é€”: å†³å®šæ˜¯å¦åœ¨åºåˆ—çš„å¼€å¤´è‡ªåŠ¨æ·»åŠ å¼€å§‹ç¬¦ï¼ˆBOSï¼Œå³ Beginning Of Sentenceï¼‰æ ‡è®°ã€‚\n",
    "- æ³¨æ„: å¹¶éžæ‰€æœ‰æ¨¡åž‹éƒ½ä½¿ç”¨ BOS æ ‡è®°ï¼Œè¿™ä¹Ÿä¾èµ–äºŽç‰¹å®šæ¨¡åž‹çš„é¢„è®­ç»ƒè¿‡ç¨‹ã€‚\n",
    "\n",
    "**use_fast (é»˜è®¤ä¸º True):**\n",
    "\n",
    "- ç”¨é€”: å†³å®šæ˜¯å¦ä½¿ç”¨å¿«é€Ÿæ ‡è®°åŒ–å™¨ã€‚å¿«é€Ÿæ ‡è®°åŒ–å™¨ï¼ˆé€šå¸¸åŸºäºŽ Rust å®žçŽ°ï¼‰æ¯”çº¯ Python æ ‡è®°åŒ–å™¨æ€§èƒ½æ›´å¥½ï¼Œä½†åœ¨ä¸€äº›ç‰¹æ®Šæƒ…å†µä¸‹å¯èƒ½éœ€è¦å…³é—­ï¼ˆä¾‹å¦‚ï¼Œå½“å¿«é€Ÿæ ‡è®°åŒ–å™¨ä¸æ”¯æŒæŸäº›ç‰¹å®šåŠŸèƒ½æ—¶ï¼‰ã€‚\n",
    "- ç¤ºä¾‹: å¦‚æžœé‡åˆ°ä¸Žå¿«é€Ÿæ ‡è®°åŒ–å™¨å…¼å®¹æ€§çš„é—®é¢˜ï¼Œå¯ä»¥è®¾ç½®ä¸º False ä½¿ç”¨çº¯ Python å®žçŽ°\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. zero shot inferencing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(model,p, maxlen=100, sample=True):\n",
    "    tokens = tokenizer(p, return_tensors=\"pt\")\n",
    "    # model.generate: ä½¿ç”¨æ¨¡åž‹è¿›è¡Œæ–‡æœ¬ç”Ÿæˆã€‚\n",
    "    res = model.generate(**tokens.to(\"cuda\"), # toks.to(\"cuda\"): å°† token æ•°æ®ç§»åŠ¨åˆ° GPU ä¸Šè¿›è¡ŒåŠ é€Ÿè®¡ç®—ã€‚\n",
    "                         max_new_tokens=maxlen,  # max_new_tokens=maxlen: ç”Ÿæˆçš„æœ€å¤§æ–° token æ•°é‡ã€‚\n",
    "                         do_sample=sample, # do_sample=sample: æ˜¯å¦ä½¿ç”¨é‡‡æ ·ç­–ç•¥ç”Ÿæˆæ–‡æœ¬ã€‚\n",
    "                         num_return_sequences=1, # num_return_sequences=1: ç”Ÿæˆçš„åºåˆ—æ•°é‡ã€‚\n",
    "                         temperature=0.1, # temperature=0.1: æŽ§åˆ¶ç”Ÿæˆå¤šæ ·æ€§çš„æ¸©åº¦å‚æ•°ã€‚\n",
    "                         num_beams=1, # num_beams=1: æŸæœç´¢çš„å¤§å°ï¼Œè¿™é‡Œè®¾ç½®ä¸º 1 è¡¨ç¤ºä¸ä½¿ç”¨æŸæœç´¢ã€‚\n",
    "                         top_p=0.95, # top_p=0.95: ä½¿ç”¨æ ¸é‡‡æ ·ç­–ç•¥ï¼Œåªè€ƒè™‘ç´¯è®¡æ¦‚çŽ‡è‡³å°‘ä¸º 95% çš„æœ€é«˜æ¦‚çŽ‡ tokenã€‚\n",
    "                        ).to('cpu')\n",
    "  \n",
    "    return tokenizer.batch_decode(res, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "Instruct: Summarize the following conversation.\n",
      "#Person1#: Happy Birthday, this is for you, Brian.\n",
      "#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n",
      "#Person1#: Brian, may I have a pleasure to have a dance with you?\n",
      "#Person2#: Ok.\n",
      "#Person1#: This is really wonderful party.\n",
      "#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n",
      "#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n",
      "#Person2#: You look great, you are absolutely glowing.\n",
      "#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\n",
      "Output:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "Person1 and Person2 are at a party, and Person1 asks if they can have a dance. Person2 agrees and compliments Person1 on their appearance. Person1 thanks them and expresses their happiness with the party. Person2 agrees that it's a great party and suggests having a drink to celebrate.\n",
      "\n",
      "CPU times: user 3.35 s, sys: 327 ms, total: 3.68 s\n",
      "Wall time: 4.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import set_seed\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "index = 10\n",
    "prompt = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "formatted_prompt = f\"Instruct: Summarize the following conversation.\\n{prompt}\\nOutput:\\n\"\n",
    "res = gen(original_model,formatted_prompt,100,)\n",
    "#print(res[0])\n",
    "output = res[0].split('Output:\\n')[1]\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{formatted_prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Pre-processing dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset cannot be directly employed for fine-tuning. It is essential to format the prompt in a way that the model can comprehend. Referring to the HuggingFace model documentation, it is evident that a prompt needs to be generated using dialogue and summary in the specified format below.\n",
    "\n",
    "we need to convert the dialog-summary (prompt-response) pairs into explicit instructions for the LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_formats(sample):\n",
    "    \"\"\"\n",
    "    Format various fields of the sample ('instruction','output')\n",
    "    Then concatenate them using two newline characters \n",
    "    :param sample: Sample dictionnary\n",
    "    \"\"\"\n",
    "    INTRO_BLURB = \"Below is an instruction that describles a task. Write a response that appropriately completes the request.\"\n",
    "    INSTRUCTION_KEY = \"Instruct: Summarize the below conversation.\"\n",
    "    RESPONSE_KEY = \"### Output:\"\n",
    "    END_KEY = \"### END\"\n",
    "    \n",
    "    blurb = f\"\\n{INTRO_BLURB}\"\n",
    "    instruction = f\"{INSTRUCTION_KEY}\"\n",
    "    input_context = f\"{sample['dialogue']}\" if sample[\"dialogue\"] else None\n",
    "    response = f\"{RESPONSE_KEY}\\n{sample['summary']}\"\n",
    "    end = f\"{END_KEY}\"\n",
    "    \n",
    "    # æ ¹æ®ç»™å®šçš„æ¨¡æ¿å’Œæ•°æ®æž„å»ºä¸€ä¸ªæ–‡æœ¬å—åˆ—è¡¨ï¼Œæ¯ä¸ªéƒ¨åˆ†éƒ½å¯èƒ½æ ¹æ®å…¶å†…å®¹æ˜¯å¦å­˜åœ¨æ¥å†³å®šæ˜¯å¦åŒ…å«åœ¨æœ€ç»ˆçš„åˆ—è¡¨ä¸­ã€‚\n",
    "    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n",
    "    \n",
    "    formatted_prompt = '\\n\\n'.join(parts)\n",
    "    sample['text'] = formatted_prompt\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will use our model tokenizer to process these prompts into tokenized ones.\n",
    "\n",
    "Our aim here is to generate input sequences with consistent lengths, which is beneficial for fine-tuning the language model by optimizing efficiency and minimizing computational overhead. It is essential to ensure that these sequences do not surpass the modelâ€™s maximum token limit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def get_max_length(model):\n",
    "    conf = model.config\n",
    "    max_length = None\n",
    "    for length_setting in ['n_positions', 'max_position_embeddings', 'seq_length']:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            break\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "    return max_length\n",
    "\n",
    "\n",
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizing a batch\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        batch['text'],\n",
    "        max_length = max_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "    \n",
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset):\n",
    "    \"\"\"Format & tokenize it so it is ready for training\n",
    "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
    "    \"\"\"\n",
    "    # Add prompt to each sample\n",
    "    dataset = dataset.map(create_prompt_formats)\n",
    "    \n",
    "    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=['id', 'topic', 'dialogue', 'summary'],\n",
    "    )\n",
    "    # ðŸ’¡ notice: tokenizer çš„å¤„ç†ç»“æžœå¦‚ `input_ids` ç­‰å­—æ®µä¼šæ·»åŠ åˆ° sample å¯¹è±¡ä¸Šã€‚\n",
    "    \n",
    "    # Filter out samples that have input_ids exceeding max_length\n",
    "    dataset = dataset.filter(lambda sample: len(sample['input_ids']) < max_length)\n",
    "    \n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n"
     ]
    }
   ],
   "source": [
    "max_length = get_max_length(original_model)\n",
    "print(max_length)\n",
    "\n",
    "train_dataset = preprocess_dataset(tokenizer, max_length, seed, dataset['train'])\n",
    "eval_dataset = preprocess_dataset(tokenizer, max_length, seed, dataset['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Training: (1999, 3)\n",
      "Validation: (499, 3)\n",
      "Dataset({\n",
      "    features: ['text', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 1999\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {train_dataset.shape}\")\n",
    "print(f\"Validation: {eval_dataset.shape}\")\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Preparing the model for QLoRA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's perform Parameter Efficient Fine-Tuning (PEFT) fine-tuning. PEFT is a form of instruction fine-tuning that is much more efficient than full fine-tuning. PEFT is a generic term that includes Low-Rank Adaptation (LoRA) and prompt tuning (which is NOT THE SAME as prompt engineering!). In most cases, when someone says PEFT, they typically mean LoRA. LoRA, in essence, enables efficient model fine-tuning using fewer computational resources, often achievable with just a single GPU. Following LoRA fine-tuning for a specific task or use case, the outcome is an unchanged original LLM and the emergence of a considerably smaller \"LoRA adapter,\" often representing a single-digit percentage of the original LLM size (in MBs rather than GBs).\n",
    "\n",
    "During inference, the LoRA adapter must be combined with its original LLM. The advantage lies in the ability of many LoRA adapters to reuse the original LLM, thereby reducing overall memory requirements when handling multiple tasks and use cases.\n",
    "\n",
    "Note the rank (r) hyper-parameter, which defines the rank/dimension of the adapter to be trained. r is the rank of the low-rank matrix used in the adapters, which thus controls the number of parameters trained. A higher rank will allow for more expressivity, but there is a compute tradeoff.\n",
    "\n",
    "alpha is the scaling factor for the learned weights. The weight matrix is scaled by alpha/r, and thus a higher value for alpha assigns more weight to the LoRA activations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 262364160\n",
      "all model parameters: 1521392640\n",
      "percentage of trainable model parameters: 17.24%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for  _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    \n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhiForCausalLM(\n",
      "  (model): PhiModel(\n",
      "    (embed_tokens): Embedding(51200, 2560)\n",
      "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x PhiDecoderLayer(\n",
      "        (self_attn): PhiAttention(\n",
      "          (q_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
      "          (k_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
      "          (v_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
      "          (dense): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
      "          (rotary_emb): PhiRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): PhiMLP(\n",
      "          (activation_fn): NewGELUActivation()\n",
      "          (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n",
      "          (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n",
      "        )\n",
      "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(original_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Embedding å±‚\n",
    "\n",
    "   - embed_tokens: è¯æ±‡è¡¨å¤§å°ä¸º 51200, æ¯ä¸ªè¯è¢«è¡¨ç¤ºä¸ºä¸€ä¸ª 2560 ç»´çš„å‘é‡ã€‚\n",
    "   - embed_dropoutï¼š å‡å°‘è¿‡æ‹Ÿåˆï¼Œé€šè¿‡éšæœºå°†è¾“å…¥å‘é‡çš„éƒ¨åˆ†å…ƒç´ ç½®é›¶æ¥å®žçŽ°ã€‚\n",
    "\n",
    "2. ç¼–ç å±‚ (PhiDecoderLayer) å…±æœ‰ 32 å±‚\n",
    "   - PhiAttentionï¼šrotary_emb - æ—‹è½¬ä½ç½®ç¼–ç \n",
    "   - PhiMLPï¼šå‰é¦ˆç½‘ç»œï¼ˆFeed Forward Network, FFNï¼‰å±‚ï¼ˆå¤šå±‚æ„ŸçŸ¥æœºï¼‰\n",
    "   - resid_dropout: æ®‹å·®è¿žæŽ¥åŽçš„ dropoutï¼Œç”¨äºŽå‡å°‘è¿‡æ‹Ÿåˆã€‚\n",
    "3. è¾“å‡ºå±‚ (lm_head)\n",
    "   - ç”¨äºŽå°†è§£ç å™¨çš„è¾“å‡ºè½¬æ¢å›žè¯æ±‡è¡¨å¤§å°çš„ç»´åº¦ï¼Œä»¥ä¾¿è¿›è¡Œä¸‹ä¸€ä¸ªè¯çš„é¢„æµ‹ã€‚è¿™é‡Œçš„è¾“å‡ºç»´åº¦ä¸Žè¯æ±‡è¡¨å¤§å°ç›¸åŒï¼ˆ51200ï¼‰ï¼Œä½¿å¾—æ¯ä¸ªè¾“å‡ºå¯ä»¥é€šè¿‡ softmax å˜æ¢ä¸ºè¯æ±‡è¡¨ä¸­æ¯ä¸ªè¯çš„æ¦‚çŽ‡ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "config = LoraConfig(\n",
    "    r = 32,\n",
    "    lora_alpha = 32,\n",
    "    target_modules=[\n",
    "        'q_proj',\n",
    "        'k_proj',\n",
    "        'v_proj',\n",
    "        'dense',\n",
    "    ],\n",
    "    bias='none',\n",
    "    lora_dropout = 0.05, # Conventional\n",
    "    task_type='CAUSAL_LM',\n",
    ")\n",
    "\n",
    "# 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
    "original_model.gradient_checkpointing_enable()\n",
    "\n",
    "# 2 - Using the prepare_model_for_kbit_training method from PEFT\n",
    "original_model = prepare_model_for_kbit_training(original_model)\n",
    "peft_model = get_peft_model(original_model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™ä¸¤è¡Œä»£ç æ¶‰åŠåˆ°æ¨¡åž‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„å†…å­˜ä¼˜åŒ–æŠ€æœ¯:\n",
    "\n",
    "1. æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆGradient Checkpointingï¼‰\n",
    "\n",
    "æ¢¯åº¦æ£€æŸ¥ç‚¹æ˜¯ä¸€ç§å‡å°‘è®­ç»ƒè¿‡ç¨‹ä¸­å†…å­˜ä½¿ç”¨çš„æŠ€æœ¯ï¼Œå°¤å…¶é€‚ç”¨äºŽè®­ç»ƒå¤§åž‹ç¥žç»ç½‘ç»œã€‚å…¶åŸºæœ¬æ€æƒ³æ˜¯åœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ä¸ä¿å­˜æ‰€æœ‰ä¸­é—´æ¿€æ´»å€¼ï¼Œè€Œåªä¿å­˜é€‰å®šçš„å‡ ä¸ªã€‚åœ¨åå‘ä¼ æ’­æ—¶ï¼Œé‚£äº›æœªä¿å­˜çš„ä¸­é—´æ¿€æ´»å€¼ä¼šé€šè¿‡é‡æ–°æ‰§è¡Œå‰å‘ä¼ æ’­ä¸­çš„ç›¸åº”éƒ¨åˆ†æ¥é‡æ–°è®¡ç®—ã€‚è¿™ç§æ–¹æ³•å¯ä»¥æ˜¾è‘—å‡å°‘å­˜å‚¨ä¸­é—´çŠ¶æ€æ‰€éœ€çš„å†…å­˜ï¼Œä½†ä»£ä»·æ˜¯å¢žåŠ äº†ä¸€äº›è®¡ç®—æ—¶é—´ï¼ˆå› ä¸ºæŸäº›æ“ä½œéœ€è¦é‡æ–°è®¡ç®—ï¼‰ã€‚\n",
    "\n",
    "2. prepare_model_for_kbit_training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 20971520\n",
      "all model parameters: 1542364160\n",
      "percentage of trainable model parameters: 1.36%\n"
     ]
    }
   ],
   "source": [
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): PhiForCausalLM(\n",
      "      (model): PhiModel(\n",
      "        (embed_tokens): Embedding(51200, 2560)\n",
      "        (embed_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x PhiDecoderLayer(\n",
      "            (self_attn): PhiAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=2560, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=2560, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=2560, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (dense): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=2560, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): PhiRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): PhiMLP(\n",
      "              (activation_fn): NewGELUActivation()\n",
      "              (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n",
      "              (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n",
      "            )\n",
      "            (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# See how the model looks different now, with the LoRA adapters added:\n",
    "print(peft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "import transformers\n",
    "output_dir = './peft-dialogue-summary-training/final-checkpoint'\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    warmup_steps=1, # ç”¨äºŽé€æ¸å¢žåŠ å­¦ä¹ çŽ‡åˆ°åˆå§‹è®¾å®šå€¼çš„æ­¥éª¤æ•°ã€‚\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    max_steps=500, # è®­ç»ƒè¿‡ç¨‹ä¸­çš„æœ€å¤§æ­¥æ•°ï¼Œä¸è®ºæ•°æ®æ˜¯å¦å·²ç»å®Œå…¨éåŽ†ã€‚\n",
    "    learning_rate=2e-4,\n",
    "    optim=\"paged_adamw_8bit\", # ä¼˜åŒ–å™¨ï¼šç”¨äºŽå‡å°‘å†…å­˜ä½¿ç”¨å¹¶å¯èƒ½åŠ é€Ÿè®­ç»ƒã€‚\n",
    "    logging_steps=25, # used for tracking the progress of training\n",
    "    logging_dir=\"./logs\", # å­˜æ”¾æ—¥å¿—æ–‡ä»¶çš„ç›®å½•ã€‚\n",
    "    save_strategy=\"steps\", # æ¨¡åž‹ä¿å­˜ç­–ç•¥ã€‚\"steps\"ï¼Œè¡¨ç¤ºæŒ‰æ­¥æ•°ä¿å­˜æ¨¡åž‹ã€‚\n",
    "    save_steps=25, # æŒ‡å®šå¤šå°‘æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡åž‹ã€‚\n",
    "    evaluation_strategy=\"steps\", # è¯„ä»·ç­–ç•¥ã€‚è¿™é‡Œè®¾ç½®ä¸º\"steps\"ï¼Œæ„å‘³ç€å°†æ ¹æ®æ­¥æ•°æ¥è¿›è¡Œæ¨¡åž‹è¯„ä¼°ã€‚\n",
    "    eval_steps=25, # æŒ‡å®šå¤šå°‘æ­¥è¿›è¡Œä¸€æ¬¡è¯„ä¼°ã€‚è¿™é‡Œè®¾ç½®ä¸º25ã€‚ä¸Žsave_stepså’Œlogging_stepsä¿æŒä¸€è‡´å¯ä»¥æ–¹ä¾¿åœ°å¯¹æ¯”æ¨¡åž‹çš„æ€§èƒ½ã€‚\n",
    "    do_eval=True, # æ˜¯å¦åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œè¯„ä¼°ã€‚å¯ä»¥åŠæ—¶å‘çŽ°æ¨¡åž‹çš„è¡¨çŽ°é—®é¢˜ã€‚è¿™é‡Œè®¾ç½®ä¸ºTrueï¼Œè¡¨ç¤ºå°†è¿›è¡Œè¯„ä¼°ã€‚\n",
    "    gradient_checkpointing=True, # æ˜¯å¦å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œä»¥å‡å°‘å†…å­˜æ¶ˆè€—ã€‚è¿™é‡Œè®¾ç½®ä¸ºTrueã€‚\n",
    "    report_to=\"none\", # æŒ‡å®šè®­ç»ƒè¿‡ç¨‹ä¸­çš„æ—¥å¿—å’Œå…¶ä»–è¾“å‡ºæŠ¥å‘Šåˆ°ä½•å¤„ã€‚è¿™é‡Œè®¾ç½®ä¸º\"none\"ï¼Œæ„å‘³ç€ä¸æŠ¥å‘Šåˆ°ä»»ä½•å¤–éƒ¨æœåŠ¡æˆ–æ–‡ä»¶ã€‚\n",
    "    overwrite_output_dir = 'True', # æ˜¯å¦è¦†ç›–è¾“å‡ºç›®å½•ã€‚åœ¨å¤šæ¬¡è®­ç»ƒç›¸åŒæ¨¡åž‹æˆ–å®žéªŒæ—¶ï¼Œè®¾ç½®ä¸ºTrueå¯ä»¥é¿å…æ–‡ä»¶å†²çªã€‚\n",
    "    group_by_length=True, # æ˜¯å¦æ ¹æ®é•¿åº¦å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œåˆ†ç»„ï¼Œä»¥ä¼˜åŒ–è®­ç»ƒæ•ˆçŽ‡ã€‚è¿™é‡Œè®¾ç½®ä¸ºTrueã€‚è¿™æœ‰åŠ©äºŽå‡å°‘å¡«å……ï¼Œä»Žè€Œæé«˜æ•°æ®å¤„ç†æ•ˆçŽ‡ã€‚\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "peft_model.to(device)\n",
    "\n",
    "peft_model.config.use_cache = False\n",
    "\n",
    "peft_trainer = transformers.Trainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=peft_training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**warmup_steps**: è®­ç»ƒåˆæœŸé€æ¸å¢žåŠ åˆ°é¢„è®¾åˆå§‹å­¦ä¹ çŽ‡çš„æ­¥æ•°ã€‚é€‚å½“çš„ warmup_steps å¯ä»¥å¸®åŠ©æ¨¡åž‹åœ¨è®­ç»ƒåˆæœŸç¨³å®šä¸‹æ¥ï¼Œé˜²æ­¢æ¨¡åž‹åœ¨è®­ç»ƒå¼€å§‹é˜¶æ®µç”±äºŽå­¦ä¹ çŽ‡è¿‡é«˜è€Œå‘æ•£ã€‚é€šå¸¸ï¼Œwarmup_steps çš„è®¾ç½®å–å†³äºŽæ€»è®­ç»ƒæ­¥æ•°çš„æ¯”ä¾‹ï¼Œä¸€èˆ¬å¯ä»¥è®¾ç½®ä¸ºæ€»æ­¥æ•°çš„ 10%å·¦å³ã€‚\n",
    "\n",
    "**per_device_train_batch_size**: æ¯ä¸ªè®¾å¤‡ï¼ˆå¦‚ GPUï¼‰åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ‰¹å¤„ç†å¤§å°ã€‚è¿™é‡Œè®¾ç½®ä¸º 1ï¼Œè¡¨ç¤ºæ¯ä¸ª GPU æ¯æ¬¡å¤„ç† 1 ä¸ªæ ·æœ¬ã€‚è¾ƒå°çš„æ‰¹æ¬¡å¯ä»¥å‡å°‘å†…å­˜ä½¿ç”¨ï¼Œä½†å¯èƒ½ä¼šå½±å“æ¨¡åž‹æ€§èƒ½å’Œè®­ç»ƒé€Ÿåº¦ã€‚\n",
    "\n",
    "**gradient_accumulation_steps**: æ¯è¿›è¡Œ n æ¬¡è¿­ä»£ï¼ˆå‰å‘å’Œåå‘ä¼ æ’­ï¼‰åŽï¼Œæ‰è¿›è¡Œä¸€æ¬¡æƒé‡æ›´æ–°ã€‚æ¯æ¬¡è¿­ä»£å¤„ç†çš„æ ·æœ¬æ•°ä¸º per_device_train_batch_sizeã€‚å½“ç¡¬ä»¶èµ„æºé™åˆ¶äº†æ‰¹æ¬¡å¤§å°æ—¶ï¼Œå¯ä»¥é€šè¿‡æ¢¯åº¦ç´¯ç§¯æ¥æœ‰æ•ˆå¢žå¤§æœ‰æ•ˆçš„æ‰¹æ¬¡å¤§å°ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_training_args.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 39:05, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.573400</td>\n",
       "      <td>1.372040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.351700</td>\n",
       "      <td>1.347941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.312300</td>\n",
       "      <td>1.346270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.306900</td>\n",
       "      <td>1.335687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.262300</td>\n",
       "      <td>1.341280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.336300</td>\n",
       "      <td>1.331783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.309700</td>\n",
       "      <td>1.328976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.289000</td>\n",
       "      <td>1.326600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>1.274000</td>\n",
       "      <td>1.327307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.235600</td>\n",
       "      <td>1.326225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>1.303800</td>\n",
       "      <td>1.323804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.294000</td>\n",
       "      <td>1.323208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>1.277700</td>\n",
       "      <td>1.321593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.256500</td>\n",
       "      <td>1.322509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>1.197700</td>\n",
       "      <td>1.321563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.306600</td>\n",
       "      <td>1.320734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>1.253900</td>\n",
       "      <td>1.320740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.260600</td>\n",
       "      <td>1.320466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>1.239600</td>\n",
       "      <td>1.320353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.190600</td>\n",
       "      <td>1.320093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=1.2915986137390136, metrics={'train_runtime': 2352.0193, 'train_samples_per_second': 3.401, 'train_steps_per_second': 0.213, 'total_flos': 3.748801409197056e+16, 'train_loss': 1.2915986137390136, 'epoch': 4.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Free memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied:81619 MB.\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'original_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[43moriginal_model\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m peft_trainer\n\u001b[1;32m      4\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'original_model' is not defined"
     ]
    }
   ],
   "source": [
    "del original_model\n",
    "del peft_trainer\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied:81619 MB.\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Evaluate the Model Qualitatively (Human Evaluation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.98s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "base_model_id = 'microsoft/phi-2'\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_id,\n",
    "                                                  device_map='auto',\n",
    "                                                  quantization_config=bnb_config,\n",
    "                                                  trust_remote_code=True,\n",
    "                                                  use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True, use_fast=False)\n",
    "eval_tokenizer.pad_token = eval_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add_eos_token=True åœ¨è®­ç»ƒé˜¶æ®µçš„ä½¿ç”¨æ˜¯ä¸ºäº†æ•™ä¼šæ¨¡åž‹è¯†åˆ«å’Œç”Ÿæˆåºåˆ—ç»“æŸçš„ä¿¡å·ï¼Œè€Œåœ¨ä½¿ç”¨é˜¶æ®µé€šå¸¸ä¸éœ€è¦è¿™æ ·åšï¼Œå› ä¸ºæ¨¡åž‹å·²ç»èƒ½å¤ŸåŸºäºŽå…¶è®­ç»ƒæ¥è‡ªä¸»ç”Ÿæˆç»“æŸä¿¡å·ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(base_model, './peft-dialogue-summary-training/final-checkpoint/checkpoint-1000', torch_dtype=torch.float16, is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: Happy Birthday, this is for you, Brian.\n",
      "#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n",
      "#Person1#: Brian, may I have a pleasure to have a dance with you?\n",
      "#Person2#: Ok.\n",
      "#Person1#: This is really wonderful party.\n",
      "#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n",
      "#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n",
      "#Person2#: You look great, you are absolutely glowing.\n",
      "#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "PEFT MODEL:\n",
      "Person1 and Person2 are at a party, and Person1 asks if they can have a dance. Person2 agrees and compliments Person1 on their appearance. Person1 thanks them and expresses their happiness with the party. Person2 agrees that it's a great party and suggests having a drink to celebrate.\n",
      "\n",
      "CPU times: user 2.19 s, sys: 105 ms, total: 2.29 s\n",
      "Wall time: 2.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import set_seed\n",
    "set_seed(42)\n",
    "\n",
    "index = 10\n",
    "prompt = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "formatted_prompt = f\"Instruct: Summarize the following conversation.\\n{prompt}\\nOutput:\\n\"\n",
    "peft_model_res = gen(original_model,formatted_prompt,100,)\n",
    "peft_model_output = peft_model_res[0].split('Output:\\n')[1]\n",
    "prefix, success, result = peft_model_output.partition('# End')\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'PEFT MODEL:\\n{prefix}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Evaluate the Model Quantitatively (with ROUGE Metric)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROUGE, or Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics and a software package used for evaluating automatic summarization and machine translation software in natural language processing. The metrics compare an automatically produced summary or translation against a reference or a set of references (human-produced) summary or translation.\n",
    "\n",
    "Letâ€™s now use the ROUGE metric to quantify the validity of summarizations produced by models. It compares summarizations to a â€œbaselineâ€ summary which is usually created by a human. While itâ€™s not a perfect metric, it does indicate the overall increase in summarization effectiveness that we have accomplished by fine-tuning.\n",
    "\n",
    "To demonstrate the capability of ROUGE Metric Evaluation we will use some sample inputs to evaluate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# Perform inferences for the sample of the test dataset (only 10 dialogues and summaries to save time).\n",
    "original_model = AutoModelForCausalLM.from_pretrained(base_model_id, device_map='auto', quantization_config=bnb_config, trust_remote_code=True, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_baseline_summaries</th>\n",
       "      <th>original_model_summaries</th>\n",
       "      <th>peft_model_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n",
       "      <td>Person 1: Ms. Dawson, I need you to take a dic...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In order to prevent employees from wasting tim...</td>\n",
       "      <td>Person 1: Ms. Dawson, I need you to take a dic...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n",
       "      <td>Person 1: Ms. Dawson, I need you to take a dic...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Person2# arrives late because of traffic jam....</td>\n",
       "      <td>Person2: I'm going to stop driving to work bec...</td>\n",
       "      <td>#Person2# got stuck in traffic again and #Pers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n",
       "      <td>Person1 and Person2 are discussing the traffic...</td>\n",
       "      <td>#Person2# got stuck in traffic again and #Pers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Person2# complains to #Person1# about the tra...</td>\n",
       "      <td>Person1 and Person2 are discussing the traffic...</td>\n",
       "      <td>#Person2# got stuck in traffic again and #Pers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n",
       "      <td>Kate informed that Masha and Hero are getting ...</td>\n",
       "      <td>Masha and Hero are getting divorced. Masha tel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n",
       "      <td>Kate informed that Masha and Hero are getting ...</td>\n",
       "      <td>Masha and Hero are getting divorced. Masha tel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#Person1# and Kate talk about the divorce betw...</td>\n",
       "      <td>Kate informed that Masha and Hero are getting ...</td>\n",
       "      <td>Masha and Hero are getting divorced. Masha tel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#Person1# and Brian are at the birthday party ...</td>\n",
       "      <td>Person1 and Person2 are at a party, and Person...</td>\n",
       "      <td>#Person1# brings a birthday gift for Brian and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            human_baseline_summaries  \\\n",
       "0  Ms. Dawson helps #Person1# to write a memo to ...   \n",
       "1  In order to prevent employees from wasting tim...   \n",
       "2  Ms. Dawson takes a dictation for #Person1# abo...   \n",
       "3  #Person2# arrives late because of traffic jam....   \n",
       "4  #Person2# decides to follow #Person1#'s sugges...   \n",
       "5  #Person2# complains to #Person1# about the tra...   \n",
       "6  #Person1# tells Kate that Masha and Hero get d...   \n",
       "7  #Person1# tells Kate that Masha and Hero are g...   \n",
       "8  #Person1# and Kate talk about the divorce betw...   \n",
       "9  #Person1# and Brian are at the birthday party ...   \n",
       "\n",
       "                            original_model_summaries  \\\n",
       "0  Person 1: Ms. Dawson, I need you to take a dic...   \n",
       "1  Person 1: Ms. Dawson, I need you to take a dic...   \n",
       "2  Person 1: Ms. Dawson, I need you to take a dic...   \n",
       "3  Person2: I'm going to stop driving to work bec...   \n",
       "4  Person1 and Person2 are discussing the traffic...   \n",
       "5  Person1 and Person2 are discussing the traffic...   \n",
       "6  Kate informed that Masha and Hero are getting ...   \n",
       "7  Kate informed that Masha and Hero are getting ...   \n",
       "8  Kate informed that Masha and Hero are getting ...   \n",
       "9  Person1 and Person2 are at a party, and Person...   \n",
       "\n",
       "                                peft_model_summaries  \n",
       "0  #Person1# asks Ms. Dawson to take a dictation ...  \n",
       "1  #Person1# asks Ms. Dawson to take a dictation ...  \n",
       "2  #Person1# asks Ms. Dawson to take a dictation ...  \n",
       "3  #Person2# got stuck in traffic again and #Pers...  \n",
       "4  #Person2# got stuck in traffic again and #Pers...  \n",
       "5  #Person2# got stuck in traffic again and #Pers...  \n",
       "6  Masha and Hero are getting divorced. Masha tel...  \n",
       "7  Masha and Hero are getting divorced. Masha tel...  \n",
       "8  Masha and Hero are getting divorced. Masha tel...  \n",
       "9  #Person1# brings a birthday gift for Brian and...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dialogues = dataset['test'][0:10]['dialogue']\n",
    "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "peft_model_summaries = []\n",
    "\n",
    "for idx, dialogue in enumerate(dialogues):\n",
    "    human_baseline_text_output = human_baseline_summaries[idx]\n",
    "    prompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n",
    "    \n",
    "    original_model_res = gen(original_model,prompt,100,)\n",
    "    original_model_text_output = original_model_res[0].split('Output:\\n')[1]\n",
    "    \n",
    "    peft_model_res = gen(ft_model,prompt,100,)\n",
    "    peft_model_output = peft_model_res[0].split('Output:\\n')[1]\n",
    "    #print(peft_model_output)\n",
    "    peft_model_text_output, success, result = peft_model_output.partition('#End')\n",
    "    \n",
    "\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "    peft_model_summaries.append(peft_model_text_output)\n",
    "\n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, peft_model_summaries))\n",
    " \n",
    "df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'peft_model_summaries'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge_score in /home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages (from rouge_score) (2.1.0)\n",
      "Requirement already satisfied: nltk in /home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: numpy in /home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages (from rouge_score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in /home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages (from nltk->rouge_score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages (from nltk->rouge_score) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages (from nltk->rouge_score) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /home/liangzhu/anaconda3/envs/llm/lib/python3.11/site-packages (from nltk->rouge_score) (4.66.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.27k/6.27k [00:00<00:00, 5.82MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.2916815228033625, 'rouge2': 0.11149065967039229, 'rougeL': 0.22302258313530804, 'rougeLsum': 0.24365145780467948}\n",
      "PEFT MODEL:\n",
      "{'rouge1': 0.34209051890378883, 'rouge2': 0.13197220268881993, 'rougeL': 0.2559244701694561, 'rougeLsum': 0.26872050763009436}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "peft_model_results = rouge.compute(\n",
    "    predictions=peft_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('PEFT MODEL:')\n",
    "print(peft_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\n",
      "rouge1: 5.04%\n",
      "rouge2: 2.05%\n",
      "rougeL: 3.29%\n",
      "rougeLsum: 2.51%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n",
    "\n",
    "improvement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\n",
    "for key, value in zip(peft_model_results.keys(), improvement):\n",
    "    print(f'{key}: {value*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
