{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, AutoConfig, OPTForCausalLM\n",
    "\n",
    "model_id = \"facebook/opt-6.7b\"\n",
    "model = OPTForCausalLM.from_pretrained(model_id, load_in_8bits=True)  # Load in 8-bit\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_int8_training, LoraConfig, get_peft_model\n",
    "\n",
    "model = prepare_model_for_int8_training(model)  # Prepare model for int8 training\n",
    "\n",
    "\n",
    "## LoRA Configuration\n",
    "config = LoraConfig(\n",
    "    r=8,  # the rank of LoRA, which affects the size of the LoRA matrix\n",
    "    lora_alpha=32,  # LoRA适应的比例因子\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"out_proj\",\n",
    "        \"fc_in\",\n",
    "        \"fc_out\",\n",
    "    ],  # 指定将LoRA应用到的模型模块，通常是attention和全连接层的投影\n",
    "    lora_dropout=0.05,  # 在LoRA模块中使用的dropout率\n",
    "    bias=\"none\",  # 设置bias的使用方式，这里没有使用bias\n",
    "    task_type=\"CAUSAL_LM\",  # 任务类型，这里设置为因果(自回归）语言模型\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "## data processing\n",
    "dataset = load_dataset(\"Abirate/english_quotes\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    lambda samples: tokenizer(samples[\"quote\"]), batched=True\n",
    ")\n",
    "\n",
    "## When mlm is set to False, the model is trained to predict each token based on the previous tokens, which is known as \"Causal Language Modeling\" or \"Autoregressive Language Modeling\".\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fine-tuning\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "model_dir = \"models\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_dir}/{model_id}-lora\",  # output directory\n",
    "    per_device_train_batch_size=4,  # batch size per device during training\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,  # 启用混合精度训练，可以提高训练速度，同时减少内存使用\n",
    "    logging_steps=20,  # used for tracking the progress of training\n",
    "    max_steps=100,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "model.use_cache = False\n",
    "trainer.train()\n",
    "\n",
    "model_path = f\"{model_dir}/{model_id}-lora-int8\"\n",
    "model.save_pretrained(model_path)\n",
    "\n",
    "lora_model = trainer.model\n",
    "text = \"Two things are infinite:\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
    "out = lora_model.generate(**inputs, max_length=48)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
